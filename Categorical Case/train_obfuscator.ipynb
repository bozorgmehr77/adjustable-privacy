{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1RoAzbshrYdp61lT13za7IuD-d1-tlRSv",
     "timestamp": 1675752704985
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ7roNYI59t5"
   },
   "outputs": [],
   "source": [
    "# Adjustable Privacy - train_obfuscator.ipynb\n",
    "# - Train an obfuscator.\n",
    "# - Uses Categorical dataset UCI-Adult.\n",
    "# - Saves models after each epoch number (to google drive and locally).\n",
    "# - It can stop and resume training.\n",
    "# - Draws loss, and accuracy plots and saves them (to google drive).\n",
    "# - Also it can load models and draw plots (from google drive).\n",
    "# - You can manage notebook parameters in parser block"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from torch.utils.data import random_split\n",
    "from math import floor\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "import random\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse"
   ],
   "metadata": {
    "id": "25lKDzS46OgQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Parser\n",
    "parser = argparse.ArgumentParser(description='Adjustable Privacy - Train an obfuscator. '\n",
    "                                 + 'Uses Categorical dataset UCI-Adult. '\n",
    "                                 + 'Saves models after each epoch number (to google drive and locally). '\n",
    "                                 + 'It can stop and resume training.'\n",
    "                                 + 'Draws loss and accuracy plots and saves them (to google drive and locally). '\n",
    "                                 + 'Also it can load models and draw plots (from google drive).')\n",
    "\n",
    "parser.add_argument('--resume', default = False, help = 'Accepts \"True\" or \"False\". ')\n",
    "parser.add_argument('--last_epoch', type=int, default = 0, help = 'In case of resuming training use last saved epoch number and in case of loading a model, set to model number.')\n",
    "parser.add_argument('--save_path', type=str, required=True, help = 'Full path on your google drive to save model and plots. And also load from it. Like \"drive/MyDrive/adjustable-privacy/Models/categorical-Obfuscator/\"')\n",
    "parser.add_argument('--epoch_numbers', type=int, default = 200, help = 'Number of epochs to train model. (when you want load a model, it should set to that model number)')\n",
    "parser.add_argument('--dataset_path', type=str, default = \"\", help = 'Full path on your google drive to adult.csv. Like \"drive/MyDrive/adjustable-privacy/Datasets/\"')\n",
    "\n",
    "command_string = \"--resume False\" \\\n",
    "\" --last_epoch 0\" \\\n",
    "\" --save_path drive/MyDrive/adjustable-privacy/Models/categorical-Obfuscator/\" \\\n",
    "\" --epoch_numbers 200\" \\\n",
    "\" --dataset_path drive/MyDrive/adjustable-privacy/Datasets/\"\n",
    "\n",
    "args = parser.parse_args(command_string.split())"
   ],
   "metadata": {
    "id": "vtd1uynWpJNr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyper parameters:\n",
    "isFirstRun = args.resume=='False'\n",
    "lastRunEpochNumber = args.last_epoch\n",
    "manual_seed = 20\n",
    "learning_rate = 0.001 #0.2\n",
    "batch_size = 64\n",
    "files_not_ready = True\n",
    "dataset_folder_path = args.dataset_path\n",
    "data_dir = 'adult'\n",
    "saving_path = args.save_path\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "# Number of training epochs\n",
    "num_epochs = args.epoch_numbers"
   ],
   "metadata": {
    "id": "5Ee86GyF6h55"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ],
   "metadata": {
    "id": "JfO8lf9L7Q16"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Mount google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "1buUW87E7SPa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# download dataset and unzip\n",
    "\n",
    "if files_not_ready:\n",
    "    dataset_csv_path = dataset_folder_path + '/adult.csv'\n",
    "\n",
    "    try:\n",
    "      os.mkdir(data_dir)\n",
    "      print(\"data folder created successfully\")\n",
    "    except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))\n",
    "\n",
    "    shutil.copyfile(dataset_csv_path, data_dir + r'/adult.csv')\n",
    "\n",
    "try:\n",
    "    os.mkdir(saving_path)\n",
    "    print(\"saving_path directory created successfully\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s\" % (e.strerror))"
   ],
   "metadata": {
    "id": "xiVP3be17UTj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#PreProcess dataset:\n",
    "df = pd.read_csv(os.path.join(data_dir,'adult.csv'))\n",
    "df = df.replace({'?':np.nan})\n",
    "df = df.dropna()\n",
    "df1 = pd.get_dummies(df)\n",
    "train, test = train_test_split(df1, test_size = 0.2, random_state = 42)\n",
    "utility_train_true_labels = np.array(train[['income_<=50K','income_>50K']])\n",
    "utility_test_true_labels = np.array(test[['income_<=50K','income_>50K']])\n",
    "private_train_true_labels = np.array(train[['gender_Male', 'gender_Female']])\n",
    "private_test_true_labels = np.array(test[['gender_Male', 'gender_Female']])\n",
    "x_train = (train.drop(['income_<=50K','income_>50K','gender_Male', 'gender_Female'],axis='columns'))\n",
    "x_test = (test.drop(['income_<=50K','income_>50K','gender_Male', 'gender_Female'],axis='columns'))\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "standard_scaler.fit(x_train)\n",
    "x_train = standard_scaler.transform(x_train)\n",
    "x_test = standard_scaler.transform(x_test)"
   ],
   "metadata": {
    "id": "xHFkBUAQ7kp7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class UciAdultDataset(Dataset):\n",
    "    def __init__(self, X, Y_p, Y_u):\n",
    "        self.X = X\n",
    "        self.Y_p = Y_p\n",
    "        self.Y_u = Y_u\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y_p)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.X[idx]\n",
    "        label_p = self.Y_p[idx]\n",
    "        label_u = self.Y_u[idx]\n",
    "        data = torch.from_numpy(data)\n",
    "        label_p = torch.from_numpy(label_p)\n",
    "        label_u = torch.from_numpy(label_u)\n",
    "        return data, label_p, label_u"
   ],
   "metadata": {
    "id": "eT7xR_bw7oW-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load Datas\n",
    "train_set = UciAdultDataset(x_train, private_train_true_labels, utility_train_true_labels)\n",
    "test_set = UciAdultDataset(x_test, private_test_true_labels, utility_test_true_labels)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=workers, drop_last=True)"
   ],
   "metadata": {
    "id": "2-haQnng7pL9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
   ],
   "metadata": {
    "id": "uUKlgbpu7rvP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# custom weights initialization\n",
    "def weights_init(m):\n",
    "  classname = m.__class__.__name__\n",
    "  if classname.find('Linear') != -1:\n",
    "    nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    nn.init.constant_(m.bias.data, 0)"
   ],
   "metadata": {
    "id": "J6h4DMsT7yIy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Encoder Model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # input is 102\n",
    "        self.fllc1 = nn.Linear(102, 128)\n",
    "        self.actv1 = nn.ReLU(inplace=True)\n",
    "        self.fllc2 = nn.Linear(128, 128)\n",
    "        self.actv2 = nn.ReLU(inplace=True)\n",
    "        self.fllc4 = nn.Linear(128, 64)\n",
    "        self.actv4 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # split features: 128 -> 126 + 2\n",
    "        # first classifier:\n",
    "        self.fllc_main_features2 = nn.Linear(64, 32)\n",
    "        self.actv_main_features2 = nn.ReLU(inplace=True)\n",
    "        self.fllc_main_features3 = nn.Linear(32, 8)\n",
    "        self.actv_main_features3 = nn.ReLU(inplace=True)\n",
    "        self.fllc_main_features4 = nn.Linear(8, 2)\n",
    "        self.actv_main_features4 = nn.LogSoftmax(dim=1)\n",
    "        # other features\n",
    "        self.fllc_other_features1 = nn.Linear(64, 64)\n",
    "        self.actv_other_features1 = nn.ReLU(inplace=True)\n",
    "        self.fllc_other_features2 = nn.Linear(64, 62)\n",
    "        self.actv_other_features2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        x = self.fllc1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.fllc2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.fllc4(x)\n",
    "        x = self.actv4(x)\n",
    "        # Part 2:\n",
    "        # first classifier: \n",
    "        y1 = self.fllc_main_features2(x)\n",
    "        y1 = self.actv_main_features2(y1)\n",
    "        y1 = self.fllc_main_features3(y1)\n",
    "        y1 = self.actv_main_features3(y1)\n",
    "        y1 = self.fllc_main_features4(y1)\n",
    "        y1 = self.actv_main_features4(y1)\n",
    "        # other features\n",
    "        y3 = self.fllc_other_features1(x) \n",
    "        y3 = self.actv_other_features1(y3)\n",
    "        y3 = self.fllc_other_features2(y3) \n",
    "        y3 = self.actv_other_features2(y3)\n",
    "        return y1, y3"
   ],
   "metadata": {
    "id": "QvuXGjy577an"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Decoder Model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # input size is 64\n",
    "        self.fllc1 = nn.Linear(64, 128)\n",
    "        self.actv1 = nn.ReLU(inplace=True)\n",
    "        self.fllc2 = nn.Linear(128, 128)\n",
    "        self.actv2 = nn.ReLU(inplace=True)\n",
    "        self.fllc4 = nn.Linear(128, 102)\n",
    "        self.actv4 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fllc1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.fllc2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.fllc4(x)\n",
    "        x = self.actv4(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "ZsAq6Qrj7-fg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# AE Model\n",
    "class AEModel(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(AEModel, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.encoder = Encoder(ngpu).to(device)\n",
    "        self.decoder = Decoder(ngpu).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1, y3 = self.encoder(x)\n",
    "        y = torch.cat((y1, y3), 1)\n",
    "        x = self.decoder(y)\n",
    "        return x, y1"
   ],
   "metadata": {
    "id": "ShARYF5J7_ah"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the AE\n",
    "netAE = AEModel(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netAE = nn.DataParallel(netAE, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "netAE.apply(weights_init)"
   ],
   "metadata": {
    "id": "qTd_0X4A8Es8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# total parameters\n",
    "total_params = sum(p.numel() for p in netAE.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")"
   ],
   "metadata": {
    "id": "fvfVN9YCBf-F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.MSELoss()\n",
    "criterion_main_features = nn.NLLLoss()\n",
    "optimizer = optim.Adam(netAE.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ],
   "metadata": {
    "id": "l14oamHABkyH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function - Save:\n",
    "def save_model(name, number, model, res):\n",
    "  checkpoint = {'res': res,\n",
    "                'state_dict': model.state_dict()}\n",
    "  torch.save(checkpoint, saving_path + 'checkpoint-' + name + '-' + str(number) + '.pth')\n",
    "  return True"
   ],
   "metadata": {
    "id": "X6iZ5l4DBqHj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function - Load:\n",
    "def load_model(name, number, model, device):\n",
    "  \n",
    "  checkpoint = torch.load(saving_path + 'checkpoint-' + name + '-' + str(number) + '.pth', map_location=device)\n",
    "  res = checkpoint['res']\n",
    "  model.load_state_dict(checkpoint['state_dict'])\n",
    "  return {'model':model,\n",
    "          'res':res}"
   ],
   "metadata": {
    "id": "nL46PLcyBqki"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save Start Checkpoint\n",
    "if(isFirstRun):\n",
    "  res = {'train_losses': [],\n",
    "         'valid_losses': [],\n",
    "         'y1_train_losses': [],\n",
    "         'y1_valid_losses': [],\n",
    "         'test_mse': [],\n",
    "         'test_y1_acc': [],\n",
    "         'epoch_number': 0\n",
    "        };\n",
    "  save_model('ae', 0, netAE, res)"
   ],
   "metadata": {
    "id": "tJgC5UqPBuPU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load Last Checkpoint:\n",
    "ae_load = load_model('ae', lastRunEpochNumber, netAE, device)\n",
    "\n",
    "train_losses = ae_load['res']['train_losses']\n",
    "valid_losses = ae_load['res']['valid_losses']\n",
    "y1_train_losses = ae_load['res']['y1_train_losses']\n",
    "y1_valid_losses = ae_load['res']['y1_valid_losses']\n",
    "test_mse = ae_load['res']['test_mse']\n",
    "test_y1_acc = ae_load['res']['test_y1_acc']\n",
    "last_epoch = ae_load['res']['epoch_number']"
   ],
   "metadata": {
    "id": "bklmqQf_CEfH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_class_index(labels):\n",
    "  return labels[:,0]"
   ],
   "metadata": {
    "id": "fug5k-L-5YwV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function - training function\n",
    "def fit(netAE, train_loader, optimizer, criterion):\n",
    "    print('Training')\n",
    "    netAE.train()\n",
    "    train_loss = 0\n",
    "    y1_train_loss = 0\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, data in prog_bar:\n",
    "        inputs, labels = data[0], data[2]\n",
    "        labels = extract_class_index(labels)\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        netAE.zero_grad()\n",
    "        output, y1 = netAE(inputs)\n",
    "        err = criterion(output, inputs)\n",
    "        err_y1 = criterion_main_features(y1, labels)\n",
    "        err.backward(retain_graph=True)\n",
    "        err_y1.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_loss += err.item()\n",
    "        y1_train_loss += err_y1.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    y1_train_loss = y1_train_loss / len(train_loader)\n",
    "    return train_loss, y1_train_loss"
   ],
   "metadata": {
    "id": "nzr6AwfvCNon"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function - validation function\n",
    "def validate(netAE, valid_loader, optimizer, criterion):\n",
    "    print('Validating')\n",
    "    netAE.eval()\n",
    "    valid_loss = 0\n",
    "    y1_valid_loss = 0\n",
    "    # For each batch in the dataloader\n",
    "    prog_bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in prog_bar:\n",
    "            inputs, labels = data[0], data[2]\n",
    "            labels = extract_class_index(labels)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output, y1 = netAE(inputs)\n",
    "            err = criterion(output, inputs)\n",
    "            err_y1 = criterion_main_features(y1, labels)\n",
    "            valid_loss += err.item()\n",
    "            y1_valid_loss += err_y1.item()\n",
    "    valid_loss = valid_loss / len(valid_loader)\n",
    "    y1_valid_loss = y1_valid_loss / len(valid_loader)\n",
    "    return valid_loss, y1_valid_loss"
   ],
   "metadata": {
    "id": "psrjG1KrCpz7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calc Accuracy\n",
    "def calcAccuracyTest(netAE, test_loader):\n",
    "    print('Testing')\n",
    "    netAE.to(device)\n",
    "    print(\"Calculating Accuracy...\")\n",
    "    netAE.eval()\n",
    "    mse_loss = 0\n",
    "    y1_accuracy = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in prog_bar:\n",
    "            inputs, labels = data[0], data[2]\n",
    "            labels = extract_class_index(labels)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output, y1 = netAE(inputs)\n",
    "            mse = criterion(output, inputs)\n",
    "            ps_y1 = torch.exp(y1)\n",
    "            top_p_y1, top_class_y1 = ps_y1.topk(1, dim=1)\n",
    "            equals_y1 = top_class_y1 == labels.view(*top_class_y1.shape)\n",
    "            acc_y1 = equals_y1.sum().item()\n",
    "            mse_loss += mse.item()\n",
    "            y1_accuracy += (acc_y1 / len(equals_y1))\n",
    "    mse_loss = mse_loss / len(test_loader)\n",
    "    y1_accuracy = y1_accuracy / len(test_loader)\n",
    "    return mse_loss, y1_accuracy"
   ],
   "metadata": {
    "id": "_z7EoTXEC_nv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "netAE.to(device)\n",
    "save_every_epoch = 1\n",
    "\n",
    "start = time.time()\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(last_epoch+1, num_epochs+1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \")\n",
    "    train_loss, y1_train_loss = fit(netAE, train_loader, optimizer, criterion)\n",
    "    valid_loss, y1_valid_loss = validate(netAE, test_loader, optimizer, criterion)\n",
    "    mse_loss, y1_accuracy = calcAccuracyTest(netAE, test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    y1_train_losses.append(y1_train_loss)\n",
    "    y1_valid_losses.append(y1_valid_loss)\n",
    "    test_mse.append(mse_loss)\n",
    "    test_y1_acc.append(y1_accuracy)\n",
    "\n",
    "    res = {'train_losses': train_losses,\n",
    "           'valid_losses': valid_losses,\n",
    "           'y1_train_losses': y1_train_losses,\n",
    "           'y1_valid_losses': y1_valid_losses,\n",
    "           'test_mse': test_mse,\n",
    "           'test_y1_acc': test_y1_acc,\n",
    "           'epoch_number': epoch\n",
    "          };\n",
    "    if epoch % save_every_epoch == 0:\n",
    "      save_model('ae', epoch, netAE, res)\n",
    "\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.6f}\")\n",
    "    print(f\"Main Label Train Loss: {y1_train_loss:.6f}\")\n",
    "    print(f\"Main Label Valid Loss: {y1_valid_loss:.6f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training time: {(end-start)/60:.3f} minutes\")\n",
    "\n",
    "print('TRAINING COMPLETE')"
   ],
   "metadata": {
    "id": "dPzuiQnPDaaM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print('Loss plot...')\n",
    "\n",
    "# loss plots\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Train-Valid Loss Trend\")\n",
    "plt.plot(train_losses, color='green', label='Training Loss')\n",
    "plt.plot(test_mse, color='red', label='Validation Loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(saving_path + \"loss_plot.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "rEz917xdD107"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}