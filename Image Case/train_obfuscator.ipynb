{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDshGMkxWQFV"
   },
   "outputs": [],
   "source": [
    "# Adjustable Privacy - train_obfuscator.ipynb\n",
    "# - Train an obfuscator.\n",
    "# - Uses image dataset (CelebA).\n",
    "# - Saves models after each epoch number (to google drive and locally).\n",
    "# - It can stop and resume training.\n",
    "# - Draws loss, psnr, and accuracy plots and saves them (to google drive).\n",
    "# - Also it can load models and draw plots (from google drive).\n",
    "# - You can manage notebook parameters in parser block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8qQgkY0Whqt"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "import random\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from math import floor\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Parser\n",
    "parser = argparse.ArgumentParser(description='Adjustable Privacy - Train an obfuscator. '\n",
    "                                 + 'Uses image dataset (CelebA). '\n",
    "                                 + 'Saves models after each epoch number (to google drive and locally). '\n",
    "                                 + 'It can stop and resume training.'\n",
    "                                 + 'Draws loss, psnr, and accuracy plots and saves them (to google drive and locally). '\n",
    "                                 + 'Also it can load models and draw plots (from google drive).')\n",
    "\n",
    "parser.add_argument('--resume', default = False, help = 'Accepts \"True\" or \"False\". ')\n",
    "parser.add_argument('--last_epoch', type=int, default = 0, help = 'In case of resuming training use last saved epoch number and in case of loading a model, set to model number.')\n",
    "parser.add_argument('--target_index', type=int, required=True, help = 'gender(20), smiling(31), open mouth(21), and high cheekbone(19)')\n",
    "parser.add_argument('--save_path', type=str, required=True, help = 'Full path on your google drive to save model and plots. And also load from it. Like \"drive/MyDrive/adjustable-privacy/Models/CelebA-G-S-Obfuscator/\"')\n",
    "parser.add_argument('--epoch_numbers', type=int, default = 50, help = 'Number of epochs to train model. (when you want load a model, it should set to that model number)')\n",
    "parser.add_argument('--download_dataset', default = False, help = 'Accepts \"True\" or \"False\". Download CelebA dataset or use CelebA shared directory.')\n",
    "parser.add_argument('--dataset_path', type=str, default = \"\", help = '(if --download_dataset=False) Full path on your google drive to CelebA shared directory shortcut. Like \"drive/MyDrive/adjustable-privacy/Datasets/CelebA/\"')\n",
    "\n",
    "command_string = \"--resume False\" \\\n",
    "\" --last_epoch 0\" \\\n",
    "\" --target_index 20\" \\\n",
    "\" --save_path drive/MyDrive/adjustable-privacy/Models/CelebA-G-S-Obfuscator/\" \\\n",
    "\" --epoch_numbers 200\" \\\n",
    "\" --download_dataset False\" \\\n",
    "\" --dataset_path drive/MyDrive/adjustable-privacy/Datasets/CelebA/\"\n",
    "\n",
    "args = parser.parse_args(command_string.split())"
   ],
   "metadata": {
    "id": "ab3QtcvWfLmj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz5QbqGAW8T6"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters:\n",
    "isFirstRun = args.resume=='False'\n",
    "lastRunEpochNumber = args.last_epoch\n",
    "manual_seed = 20\n",
    "image_size = 64\n",
    "use_whole_dataset = True\n",
    "usage_percent = 1.0\n",
    "learning_rate = 0.001 #0.2\n",
    "batch_size = 64\n",
    "celeba_male_index = 20\n",
    "celeba_smiling_index = 31\n",
    "celeba_mouth_open_index = 21\n",
    "celeba_high_cheekbone_index = 19\n",
    "using_index = args.target_index\n",
    "saving_path = args.save_path\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Number of GPUs available.\n",
    "ngpu = 1\n",
    "# Size of feature maps in encoder\n",
    "nef = 64\n",
    "# Size of feature maps in decoder\n",
    "ndf = 64\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "# Number of training epochs\n",
    "num_epochs = args.epoch_numbers\n",
    "data_dir = 'celeba'\n",
    "download_dataset = args.download_dataset=='True'\n",
    "dataset_folder_path = args.dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3VlZ8lvZIms"
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6M5FAIqZQcK"
   },
   "outputs": [],
   "source": [
    "# Mount google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "632eDVH1ZJG7"
   },
   "outputs": [],
   "source": [
    "# getting dataset ready from shared directory\n",
    "if not download_dataset:\n",
    "    dataset_zip_path = dataset_folder_path + '/Img/img_align_celeba.zip'\n",
    "    list_eval_partition_path = dataset_folder_path + '/Eval/list_eval_partition.txt'\n",
    "    identity_celeba_path = dataset_folder_path + '/Anno/identity_CelebA.txt'\n",
    "    list_attr_celeba_path = dataset_folder_path + '/Anno/list_attr_celeba.txt'\n",
    "    list_bbox_celeba_path = dataset_folder_path + '/Anno/list_bbox_celeba.txt'\n",
    "    list_landmarks_align_celeba_path = dataset_folder_path + '/Anno/list_landmarks_align_celeba.txt'\n",
    "\n",
    "    try:\n",
    "      os.mkdir(data_dir)\n",
    "      print(\"data folder created successfully\")\n",
    "    except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))\n",
    "\n",
    "    shutil.copyfile(dataset_zip_path, data_dir + r'/img_align_celeba.zip')\n",
    "    shutil.copyfile(list_eval_partition_path, data_dir + r'/list_eval_partition.txt')\n",
    "    shutil.copyfile(identity_celeba_path, data_dir + r'/identity_CelebA.txt')\n",
    "    shutil.copyfile(list_attr_celeba_path, data_dir + r'/list_attr_celeba.txt')\n",
    "    shutil.copyfile(list_bbox_celeba_path, data_dir + r'/list_bbox_celeba.txt')\n",
    "    shutil.copyfile(list_landmarks_align_celeba_path, data_dir + r'/list_landmarks_align_celeba.txt')\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(data_dir + r'/img_align_celeba')\n",
    "        print(\"old unzipped directory removed successfully\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s\" % (e.strerror))\n",
    "\n",
    "    archive = data_dir + r'/img_align_celeba.zip'\n",
    "    with ZipFile(archive, 'r') as zip:\n",
    "       zip.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# make saving path dir\n",
    "try:\n",
    "    os.makedirs(saving_path)\n",
    "    print(\"saving_path directory created successfully\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s\" % (e.strerror))"
   ],
   "metadata": {
    "id": "boMiC3EOgZeU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ0HTan5cWmA"
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = transforms.Compose([transforms.Resize(image_size),\n",
    "                                       transforms.CenterCrop(image_size),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(image_size),\n",
    "                                      transforms.CenterCrop(image_size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eA7edaPcjIo"
   },
   "outputs": [],
   "source": [
    "# Function - use some percent of data\n",
    "def shorten_dataset(dataset, usage_percent=1.0):\n",
    "  len_used = floor(len(dataset)*usage_percent)\n",
    "  len_not_used = len(dataset) - len_used\n",
    "  used_dataset, not_used_dataset = random_split(dataset, [len_used, len_not_used], generator=torch.Generator().manual_seed(manual_seed)) \n",
    "  return used_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1wqKdpPdF7W"
   },
   "outputs": [],
   "source": [
    "# Load Datas\n",
    "train_set = datasets.CelebA(root='', download=download_dataset, split='train', target_type=[\"attr\", \"identity\"], transform=train_transforms)\n",
    "test_set = datasets.CelebA(root='', download=download_dataset, split='test', target_type=[\"attr\", \"identity\"], transform=test_transforms)\n",
    "valid_set = datasets.CelebA(root='', download=download_dataset, split='valid', target_type=[\"attr\", \"identity\"], transform=test_transforms)\n",
    "\n",
    "# shorten Dataset\n",
    "if not use_whole_dataset:\n",
    "  train_set = shorten_dataset(train_set, usage_percent)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=workers, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, num_workers=workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1oMlpDcr8Yv"
   },
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eu36HQMdsxSF"
   },
   "outputs": [],
   "source": [
    "# custom weights initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be0xLbL4tMKz"
   },
   "outputs": [],
   "source": [
    "# Encoder Model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # input is nc x 64 x 64 \n",
    "        self.conv1 = nn.Conv2d(nc, nef, 4, 2, 1, bias=False)\n",
    "        self.actv1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor2 = nn.BatchNorm2d(nef)\n",
    "        self.actv2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor3 = nn.BatchNorm2d(nef)\n",
    "        self.actv3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(nef, nef * 2, 4, 2, 1, bias=False)\n",
    "        self.bnor4 = nn.BatchNorm2d(nef * 2)\n",
    "        self.actv4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef*2) x 4 x 4\n",
    "        # shaping would be here: nef*2 x 4 x 4 -> 2048\n",
    "        # state size. 2048\n",
    "        self.fllc5 = nn.Linear(nef*2*4*4, nef*1*4*4)\n",
    "        self.actv5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. 1024\n",
    "\n",
    "        # split features: 1024 -> 1022 + 2\n",
    "        # classifier:\n",
    "        self.fllc_male_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_male_features1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_male_features1 = nn.Dropout(p=0.5)\n",
    "        self.fllc_male_features2 = nn.Linear(nef*1*4*4, nef*4)\n",
    "        self.actv_male_features2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_male_features2 = nn.Dropout(p=0.5)\n",
    "        self.fllc_male_features3 = nn.Linear(nef*4, nef)\n",
    "        self.actv_male_features3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.fllc_male_features4 = nn.Linear(nef, 2)\n",
    "        self.actv_male_features4 = nn.LogSoftmax(dim=1)\n",
    "        # other features\n",
    "        self.fllc_other_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_other_features1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_other_features1 = nn.Dropout(p=0.5)\n",
    "        self.fllc_other_features2 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_other_features2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_other_features2 = nn.Dropout(p=0.5)\n",
    "        self.fllc_other_features3 = nn.Linear(nef*1*4*4, nef*1*4*4 - 2)\n",
    "        self.actv_other_features3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # aggregate features for output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        x = self.conv1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnor2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bnor3(x)\n",
    "        x = self.actv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bnor4(x)\n",
    "        x = self.actv4(x)\n",
    "        # flatten\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        # Part 2:\n",
    "        x = self.fllc5(x)\n",
    "        x = self.actv5(x)\n",
    "        # first classifier:\n",
    "        y1 = self.fllc_male_features1(x)\n",
    "        y1 = self.actv_male_features1(y1)\n",
    "        y1 = self.dropout_male_features1(y1)\n",
    "        y1 = self.fllc_male_features2(y1)\n",
    "        y1 = self.actv_male_features2(y1)\n",
    "        y1 = self.dropout_male_features2(y1)\n",
    "        y1 = self.fllc_male_features3(y1)\n",
    "        y1 = self.actv_male_features3(y1)\n",
    "        y1 = self.fllc_male_features4(y1)\n",
    "        y1 = self.actv_male_features4(y1)\n",
    "        # other features\n",
    "        y3 = self.fllc_other_features1(x) \n",
    "        y3 = self.actv_other_features1(y3)\n",
    "        y3 = self.dropout_other_features1(y3)\n",
    "        y3 = self.fllc_other_features2(y3) \n",
    "        y3 = self.actv_other_features2(y3)\n",
    "        y3 = self.dropout_other_features2(y3)\n",
    "        y3 = self.fllc_other_features3(y3) \n",
    "        y3 = self.actv_other_features3(y3)\n",
    "        return y1, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiMBC0CHtNX8"
   },
   "outputs": [],
   "source": [
    "# Decoder Model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # input size is 1024\n",
    "        self.fllc6 = nn.Linear(nef*1*4*4, ndf*2*4*4)\n",
    "        self.actv6 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. 2048\n",
    "        # shaping would be here: 2048 -> ndf*2 x 4 x 4\n",
    "        # state size. (ndf*2) x 4 x 4\n",
    "        self.cnvt7 = nn.ConvTranspose2d( ndf*2, ndf, 4, 2, 1, bias=False)\n",
    "        self.bnor7 = nn.BatchNorm2d(ndf)\n",
    "        self.actv7 = nn.ReLU(True)\n",
    "        # state size. (ndf) x 8 x 8\n",
    "        self.cnvt8 = nn.ConvTranspose2d(ndf, ndf, 4, 2, 1, bias=False)\n",
    "        self.bnor8 = nn.BatchNorm2d(ndf)\n",
    "        self.actv8 = nn.ReLU(True)\n",
    "        # state size. (ndf) x 16 x 16\n",
    "        self.cnvt9 = nn.ConvTranspose2d( ndf, ndf, 4, 2, 1, bias=False)\n",
    "        self.bnor9 = nn.BatchNorm2d(ndf)\n",
    "        self.actv9 = nn.ReLU(True)\n",
    "        # state size. (ndf) x 32 x 32\n",
    "        self.cnvt10 = nn.ConvTranspose2d( ndf, nc, 4, 2, 1, bias=False)\n",
    "        self.actv10 = nn.Sigmoid() # nn.Tanh()\n",
    "        # state size. (nc) x 64 x 64 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fllc6(x)\n",
    "        x = self.actv6(x)\n",
    "        x = x.view(batch_size, ndf*2, 4, 4)\n",
    "        x = self.cnvt7(x)\n",
    "        x = self.bnor7(x)\n",
    "        x = self.actv7(x)\n",
    "        x = self.cnvt8(x)\n",
    "        x = self.bnor8(x)\n",
    "        x = self.actv8(x)\n",
    "        x = self.cnvt9(x)\n",
    "        x = self.bnor9(x)\n",
    "        x = self.actv9(x)\n",
    "        x = self.cnvt10(x)\n",
    "        x = self.actv10(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VrvmFLAtUfK"
   },
   "outputs": [],
   "source": [
    "# AE Model\n",
    "class AEModel(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(AEModel, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.encoder = Encoder(ngpu).to(device)\n",
    "        self.decoder = Decoder(ngpu).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1, y3 = self.encoder(x)\n",
    "        y = torch.cat((y1, y3), 1)\n",
    "        x = self.decoder(y)\n",
    "        return x, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZkZ6c_stXg7"
   },
   "outputs": [],
   "source": [
    "# Create the AE\n",
    "netAE = AEModel(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netAE = nn.DataParallel(netAE, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "netAE.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N8qPF5LvekF"
   },
   "outputs": [],
   "source": [
    "# total parameters\n",
    "total_params = sum(p.numel() for p in netAE.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSNZPJFSvynL"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "criterion_main_features = nn.NLLLoss()\n",
    "optimizer = optim.Adam(netAE.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k7gGE5uwP27"
   },
   "outputs": [],
   "source": [
    "# Function - Save:\n",
    "def save_model(name, number, model, res):\n",
    "  checkpoint = {'res': res,\n",
    "                'state_dict': model.state_dict()}\n",
    "  torch.save(checkpoint, saving_path + 'checkpoint-' + name + '-' + str(number) + '.pth')\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77ovq9TiwQ-S"
   },
   "outputs": [],
   "source": [
    "# Function - Load:\n",
    "def load_model(name, number, model, device):\n",
    "  \n",
    "  checkpoint = torch.load(saving_path + 'checkpoint-' + name + '-' + str(number) + '.pth', map_location=device)\n",
    "  res = checkpoint['res']\n",
    "  model.load_state_dict(checkpoint['state_dict'])\n",
    "  return {'model':model,\n",
    "          'res':res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syoSR7YCwhkB"
   },
   "outputs": [],
   "source": [
    "# Save Start Checkpoint\n",
    "if(isFirstRun):\n",
    "  res = {'train_losses': [],\n",
    "         'valid_losses': [],\n",
    "         'y1_train_losses': [],\n",
    "         'y1_valid_losses': [],\n",
    "         'test_mse': [],\n",
    "         'test_psnr': [],\n",
    "         'test_y1_acc': [],\n",
    "         'epoch_number': 0\n",
    "        };\n",
    "  save_model('ae', 0, netAE, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-YMhj_Pwsyo"
   },
   "outputs": [],
   "source": [
    "# Load Last Checkpoint:\n",
    "ae_load = load_model('ae', lastRunEpochNumber, netAE, device)\n",
    "\n",
    "train_losses = ae_load['res']['train_losses']\n",
    "valid_losses = ae_load['res']['valid_losses']\n",
    "y1_train_losses = ae_load['res']['y1_train_losses']\n",
    "y1_valid_losses = ae_load['res']['y1_valid_losses']\n",
    "test_mse = ae_load['res']['test_mse']\n",
    "test_psnr = ae_load['res']['test_psnr']\n",
    "test_y1_acc = ae_load['res']['test_y1_acc']\n",
    "last_epoch = ae_load['res']['epoch_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fak16-LDxi4e"
   },
   "outputs": [],
   "source": [
    "# Function - training function\n",
    "def fit(netAE, train_loader, optimizer, criterion):\n",
    "    print('Training')\n",
    "    netAE.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    y1_train_loss = 0\n",
    "    # For each batch in the dataloader\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, data in prog_bar:\n",
    "        inputs, labels = data[0], data[1]\n",
    "        inputs = inputs.to(device)\n",
    "        main_target = labels[0][:, using_index]\n",
    "        main_target = main_target.to(device)\n",
    "        netAE.zero_grad()\n",
    "        output, y1 = netAE(inputs)\n",
    "        err = criterion(output, inputs)\n",
    "        err_y1 = criterion_main_features(y1, main_target)\n",
    "        err.backward(retain_graph=True)\n",
    "        err_y1.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_loss += err.item()\n",
    "        y1_train_loss += err_y1.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    y1_train_loss = y1_train_loss / len(train_loader)\n",
    "    return train_loss, y1_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onkOgwrlxnzl"
   },
   "outputs": [],
   "source": [
    "# Function - validation function\n",
    "def validate(netAE, valid_loader, optimizer, criterion):\n",
    "    print('Validating')\n",
    "    netAE.eval()\n",
    "\n",
    "    valid_loss = 0\n",
    "    y1_valid_loss = 0\n",
    "    # For each batch in the dataloader\n",
    "    prog_bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in prog_bar:\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.to(device)\n",
    "            main_target = labels[0][:, using_index]\n",
    "            main_target = main_target.to(device)\n",
    "            output, y1 = netAE(inputs)\n",
    "            err = criterion(output, inputs)\n",
    "            err_y1 = criterion_main_features(y1, main_target)\n",
    "            valid_loss += err.item()\n",
    "            y1_valid_loss += err_y1.item()\n",
    "    valid_loss = valid_loss / len(valid_loader)\n",
    "    y1_valid_loss = y1_valid_loss / len(valid_loader)\n",
    "    return valid_loss, y1_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zx8yze8CVFe"
   },
   "outputs": [],
   "source": [
    "def _reduce(x: torch.Tensor, reduction: str = 'mean') -> torch.Tensor:\n",
    "    r\"\"\"Reduce input in batch dimension if needed.\n",
    "    Args:\n",
    "        x: Tensor with shape (N, *).\n",
    "        reduction: Specifies the reduction type:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``\n",
    "    \"\"\"\n",
    "    if reduction == 'none':\n",
    "        return x\n",
    "    elif reduction == 'mean':\n",
    "        return x.mean(dim=0)\n",
    "    elif reduction == 'sum':\n",
    "        return x.sum(dim=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown reduction. Expected one of {'none', 'mean', 'sum'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwCb7SmWCVFe"
   },
   "outputs": [],
   "source": [
    "# Function PSNR:\n",
    "from typing import Union\n",
    "def psnr_calc(x: torch.Tensor, y: torch.Tensor, data_range: Union[int, float] = 1.0,\n",
    "         reduction: str = 'mean', convert_to_greyscale: bool = False) -> torch.Tensor:\n",
    "    r\"\"\"Compute Peak Signal-to-Noise Ratio for a batch of images.\n",
    "    Supports both greyscale and color images with RGB channel order.\n",
    "    Args:\n",
    "        x: An input tensor. Shape :math:`(N, C, H, W)`.\n",
    "        y: A target tensor. Shape :math:`(N, C, H, W)`.\n",
    "        data_range: Maximum value range of images (usually 1.0 or 255).\n",
    "        reduction: Specifies the reduction type:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. Default:``'mean'``\n",
    "        convert_to_greyscale: Convert RGB image to YIQ format and computes PSNR\n",
    "            only on luminance channel if `True`. Compute on all 3 channels otherwise.\n",
    "    Returns:\n",
    "        PSNR Index of similarity between two images.\n",
    "    References:\n",
    "        https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n",
    "    \"\"\"\n",
    "    #_validate_input([x, y], dim_range=(4, 5), data_range=(0, data_range))\n",
    "\n",
    "    # Constant for numerical stability\n",
    "    EPS = 1e-8\n",
    "\n",
    "    x = x / float(data_range)\n",
    "    y = y / float(data_range)\n",
    "\n",
    "    if (x.size(1) == 3) and convert_to_greyscale:\n",
    "        # Convert RGB image to YIQ and take luminance: Y = 0.299 R + 0.587 G + 0.114 B\n",
    "        rgb_to_grey = torch.tensor([0.299, 0.587, 0.114]).view(1, -1, 1, 1).to(x)\n",
    "        x = torch.sum(x * rgb_to_grey, dim=1, keepdim=True)\n",
    "        y = torch.sum(y * rgb_to_grey, dim=1, keepdim=True)\n",
    "\n",
    "    mse = torch.mean((x - y) ** 2, dim=[1, 2, 3])\n",
    "    score: torch.Tensor = - 10 * torch.log10(mse + EPS)\n",
    "\n",
    "    return _reduce(score, reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR0MxMoDCVFf"
   },
   "outputs": [],
   "source": [
    "# Calc Accuracy\n",
    "def calcAccuracyTest(netAE, test_loader):\n",
    "    print('Testing')\n",
    "    netAE.to(device)\n",
    "    print(\"Calculating Accuracy...\")\n",
    "    netAE.eval()\n",
    "    mse_loss = 0\n",
    "    psnr_loss = 0\n",
    "    y1_accuracy = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in prog_bar:\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.to(device)\n",
    "            main_target = labels[0][:, using_index]\n",
    "            main_target = main_target.to(device)\n",
    "            output, y1 = netAE(inputs)\n",
    "            mse = criterion(output, inputs)\n",
    "            psnr = psnr_calc(output, inputs)\n",
    "            ps_y1 = torch.exp(y1)\n",
    "            top_p_y1, top_class_y1 = ps_y1.topk(1, dim=1)\n",
    "            equals_y1 = top_class_y1 == main_target.view(*top_class_y1.shape)\n",
    "            acc_y1 = equals_y1.sum().item()\n",
    "            mse_loss += mse.item()\n",
    "            psnr_loss += psnr.item()\n",
    "            y1_accuracy += (acc_y1 / len(equals_y1))\n",
    "    mse_loss = mse_loss / len(test_loader)\n",
    "    psnr_loss = psnr_loss / len(test_loader)\n",
    "    y1_accuracy = y1_accuracy / len(test_loader)\n",
    "    return mse_loss, psnr_loss, y1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kn8-cvwFxpwV"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "netAE.to(device)\n",
    "save_every_epoch = 1\n",
    "\n",
    "start = time.time()\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(last_epoch+1, num_epochs+1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \")\n",
    "    train_loss, y1_train_loss = fit(netAE, train_loader, optimizer, criterion)\n",
    "    valid_loss, y1_valid_loss = validate(netAE, valid_loader, optimizer, criterion)\n",
    "    mse_loss, psnr_loss, y1_accuracy = calcAccuracyTest(netAE, test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    y1_train_losses.append(y1_train_loss)\n",
    "    y1_valid_losses.append(y1_valid_loss)\n",
    "    test_mse.append(mse_loss)\n",
    "    test_psnr.append(psnr_loss)\n",
    "    test_y1_acc.append(y1_accuracy)\n",
    "\n",
    "    res = {'train_losses': train_losses,\n",
    "           'valid_losses': valid_losses,\n",
    "           'y1_train_losses': y1_train_losses,\n",
    "           'y1_valid_losses': y1_valid_losses,\n",
    "           'test_mse': test_mse,\n",
    "           'test_psnr': test_psnr,\n",
    "           'test_y1_acc': test_y1_acc,\n",
    "           'epoch_number': epoch\n",
    "          };\n",
    "    if epoch % save_every_epoch == 0:\n",
    "      save_model('ae', epoch, netAE, res)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f}\")\n",
    "    print(f\"Main Label Train Loss: {y1_train_loss:.4f}\")\n",
    "    print(f\"Main Label Valid Loss: {y1_valid_loss:.4f}\")\n",
    "    print(f\"Main Label Accuracy on Testset: {y1_accuracy:.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training time: {(end-start)/60:.3f} minutes\")\n",
    "\n",
    "print('TRAINING COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install LATEX\n",
    "!sudo apt-get update\n",
    "!sudo apt install cm-super dvipng texlive-latex-extra texlive-latex-recommended texlive texlive-fonts-recommended\n",
    "import os\n",
    "from matplotlib.pyplot import text\n",
    "matplotlib.style.use('default')\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('font', **{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)"
   ],
   "metadata": {
    "id": "nPUriOE6mebp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# e2e Train Loss and PSNR Plot\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "print('Loss and PSNR plot...')\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "fig.tight_layout()\n",
    "ax=fig.add_subplot(111, label=\"1\")\n",
    "ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "lns1 = ax.plot(train_losses, color='green', label='Training Loss')\n",
    "lns2 = ax.plot(test_mse, color='blue', label='Validation Loss')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE Loss\")\n",
    "ax.set_xlim([0,200])\n",
    "\n",
    "lns3 = ax2.plot(test_psnr, color='black', label='PSNR')\n",
    "ax2.set_ylabel(\"PSNR (dB)\")\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.yaxis.set_label_position('right') \n",
    "ax2.set_xlim([0,200])\n",
    "ax2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "leg = lns1 + lns2 + lns3\n",
    "labs = [l.get_label() for l in leg]\n",
    "ax.legend(leg, labs, frameon=True, loc=5)\n",
    "\n",
    "plt.grid(color = 'gray', linestyle = 'dotted', linewidth = 0.5)\n",
    "plt.savefig(saving_path + \"e2e_psnr_and_loss_plot.svg\", bbox_inches = 'tight')\n",
    "plt.savefig(saving_path + \"e2e_psnr_and_loss_plot.png\", bbox_inches = 'tight')\n",
    "plt.savefig(saving_path + \"e2e_psnr_and_loss_plot.eps\", bbox_inches = 'tight', format='eps')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "QGShdOoomxCJ"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1Xop940JebvKrV4bSyqSi6ZQd_z-eneYL",
     "timestamp": 1674694846821
    },
    {
     "file_id": "1A-jB4FqePiVpaWcM2wtchkyzrWxFSRN8",
     "timestamp": 1670199525683
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}