{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"CDshGMkxWQFV","executionInfo":{"status":"ok","timestamp":1681611359789,"user_tz":-210,"elapsed":377,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Adjustable Privacy - decorrelation_test.ipynb\n","# - Uses image datasets (CelebA).\n","# - This code creates random face pictures by permuting some picture features' and set their utility label to specific value.\n","# - Then, evaluate a well trained machine to infer utility label from them.\n","# - report accuracy and histogram of output node for both 0 and 1 utility labels\n","# - draw some visually manipulated faces in terms of utility label\n","# - You can manage notebook parameters in parser block"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"C8qQgkY0Whqt","executionInfo":{"status":"ok","timestamp":1681611365373,"user_tz":-210,"elapsed":4568,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Imports\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data import random_split\n","from torchvision import datasets, transforms, models\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","matplotlib.style.use('ggplot')\n","import itertools\n","import random\n","import shutil\n","from zipfile import ZipFile\n","import os\n","from tqdm import tqdm\n","from collections import OrderedDict\n","import time\n","from math import floor\n","import argparse"]},{"cell_type":"code","source":["# Parser\n","parser = argparse.ArgumentParser(description='Adjustable Privacy '\n","                                 + 'Uses image dataset (CelebA). '\n","                                 + 'This code creates random face pictures by permuting some picture features\\' and set their utility label to specific value. '\n","                                 + 'Then, evaluate a well trained machine to infer utility label from them. '\n","                                 + 'report accuracy and histogram of output node for both 0 and 1 utility labels. '\n","                                 + 'draw some visually manipulated faces in terms of utility label ' )\n","\n","parser.add_argument('--obf_model_number', type=int, required=True, help = 'The epoch number of desired obfuscator model (model number)')\n","parser.add_argument('--utl_model_number', type=int, required=True, help = 'The epoch number of desired trained model (infer target label - smiling, open mouth, and highcheekbone)')\n","parser.add_argument('--target_index', type=int, required=True, help = 'smiling(31), open mouth(21), and high cheekbone(19)')\n","parser.add_argument('--obf_load_path', type=str, required=True, help = 'Full path on your google drive to load obfuscator model from. Like \"drive/MyDrive/adjustable-privacy/Models/CelebA-G-S-Obfuscator/\"')\n","parser.add_argument('--utl_load_path', type=str, required=True, help = 'Full path on your google drive to load trained model (infer target label - smiling, open mouth, and highcheekbone) from. Like \"drive/MyDrive/adjustable-privacy/Models/CelebA-S/\"')\n","parser.add_argument('--save_path', type=str, required=True, help = 'Full path on your google drive to save results. Like \"drive/MyDrive/adjustable-privacy/Others/Results/\"')\n","parser.add_argument('--download_dataset', default = False, help = 'Accepts \"True\" or \"False\". Download CelebA dataset or use CelebA shared directory.')\n","parser.add_argument('--dataset_path', type=str, default = \"\", help = '(if --download_dataset=False) Full path on your google drive to CelebA shared directory shortcut. Like \"drive/MyDrive/adjustable-privacy/Datasets/CelebA/\"')\n","parser.add_argument('--result_file_name', type=str, default = \"\", help = 'a short name specify the result file name like:\"GS-Hist-ModelNumber183-g3000-f0\" ')\n","parser.add_argument('--use_g', required=True, help = 'Accepts \"True\" or \"False\". Activate g function or not.')\n","parser.add_argument('--lambda_v', type=int, default = -3000, help = 'Value of lambda (only when use_g=True)')\n","parser.add_argument('--noise', type=int, default = 0, help = 'Value of noise coeficient.')\n","\n","command_string = \"--obf_model_number 183\" \\\n","\" --utl_model_number 6\" \\\n","\" --target_index 20\" \\\n","\" --obf_load_path drive/MyDrive/adjustable-privacy/Models/CelebA-G-S-Obfuscator/\" \\\n","\" --utl_load_path drive/MyDrive/adjustable-privacy/Models/CelebA-S/\" \\\n","\" --save_path drive/MyDrive/adjustable-privacy/Others/Results/\" \\\n","\" --download_dataset False\" \\\n","\" --dataset_path drive/MyDrive/adjustable-privacy/Datasets/CelebA/\" \\\n","\" --result_file_name GS-Hist-ModelNumber183-g3000-f0\" \\\n","\" --use_g True\" \\\n","\" --lambda_v -3000\" \\\n","\" --noise 0\"\n","\n","args = parser.parse_args(command_string.split())"],"metadata":{"id":"4x2J1UaIxI1U","executionInfo":{"status":"ok","timestamp":1681611365374,"user_tz":-210,"elapsed":7,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo apt install cm-super dvipng texlive-latex-extra texlive-latex-recommended texlive texlive-fonts-recommended\n","import os\n","from matplotlib.pyplot import text\n","matplotlib.style.use('default')\n","from matplotlib import rc\n","\n","rc('font', **{'family':'serif','serif':['Palatino']})\n","rc('text', usetex=True)"],"metadata":{"id":"ozrUtZu46OY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Wz5QbqGAW8T6","executionInfo":{"status":"ok","timestamp":1681611430679,"user_tz":-210,"elapsed":24,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Hyper parameters:\n","manual_seed = 20\n","image_size = 64\n","use_whole_dataset = True\n","usage_percent = 1.0\n","celeba_male_index = 20\n","celeba_smiling_index = 31\n","celeba_mouth_open_index = 21\n","celeba_high_cheekbone_index = 19\n","learning_rate = 0.001 #0.2\n","batch_size = 1 #64\n","\n","p2r_model_number = args.obf_model_number\n","utl_model_number = args.utl_model_number\n","\n","p2r_model_path = args.obf_load_path\n","utl_model_path = args.utl_load_path\n","\n","saving_path = args.save_path\n","\n","files_not_ready = True\n","download_dataset = args.download_dataset=='True'\n","dataset_folder_path = args.dataset_path\n","data_dir = 'celeba'\n","\n","#----------------\n","using_index = args.target_index\n","use_g = args.use_g=='True'\n","g_eff_val = args.lambda_v\n","miu = 0\n","coef_for_var = args.noise\n","\n","# Number of workers for dataloader\n","workers = 2\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","# Size of feature maps in encoder\n","nef = 64\n","# Size of feature maps in decoder\n","ndf = 64\n","# Number of channels in the training images. For color images this is 3\n","nc = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3VlZ8lvZIms"},"outputs":[],"source":["# Check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6M5FAIqZQcK"},"outputs":[],"source":["# Mount google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"632eDVH1ZJG7"},"outputs":[],"source":["# getting dataset ready from shared directory\n","if not download_dataset:\n","    dataset_zip_path = dataset_folder_path + '/Img/img_align_celeba.zip'\n","    list_eval_partition_path = dataset_folder_path + '/Eval/list_eval_partition.txt'\n","    identity_celeba_path = dataset_folder_path + '/Anno/identity_CelebA.txt'\n","    list_attr_celeba_path = dataset_folder_path + '/Anno/list_attr_celeba.txt'\n","    list_bbox_celeba_path = dataset_folder_path + '/Anno/list_bbox_celeba.txt'\n","    list_landmarks_align_celeba_path = dataset_folder_path + '/Anno/list_landmarks_align_celeba.txt'\n","\n","    try:\n","      os.mkdir(data_dir)\n","      print(\"data folder created successfully\")\n","    except OSError as e:\n","      print(\"Error: %s\" % (e.strerror))\n","\n","    shutil.copyfile(dataset_zip_path, data_dir + r'/img_align_celeba.zip')\n","    shutil.copyfile(list_eval_partition_path, data_dir + r'/list_eval_partition.txt')\n","    shutil.copyfile(identity_celeba_path, data_dir + r'/identity_CelebA.txt')\n","    shutil.copyfile(list_attr_celeba_path, data_dir + r'/list_attr_celeba.txt')\n","    shutil.copyfile(list_bbox_celeba_path, data_dir + r'/list_bbox_celeba.txt')\n","    shutil.copyfile(list_landmarks_align_celeba_path, data_dir + r'/list_landmarks_align_celeba.txt')\n","\n","    try:\n","        shutil.rmtree(data_dir + r'/img_align_celeba')\n","        print(\"old unzipped directory removed successfully\")\n","    except OSError as e:\n","        print(\"Error: %s\" % (e.strerror))\n","\n","    archive = data_dir + r'/img_align_celeba.zip'\n","    with ZipFile(archive, 'r') as zip:\n","       zip.extractall(data_dir)"]},{"cell_type":"code","source":["# make saving path dir\n","try:\n","    os.makedirs(saving_path)\n","    print(\"saving_path directory created successfully\")\n","except OSError as e:\n","    print(\"Error: %s\" % (e.strerror))"],"metadata":{"id":"0gFLCo1I0E8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"NJ0HTan5cWmA","executionInfo":{"status":"ok","timestamp":1681611507693,"user_tz":-210,"elapsed":4,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Define transforms\n","train_transforms = transforms.Compose([transforms.Resize(image_size),\n","                                       transforms.CenterCrop(image_size),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.485, 0.456, 0.406],\n","                                                            [0.229, 0.224, 0.225])])\n","\n","test_transforms = transforms.Compose([transforms.Resize(image_size),\n","                                      transforms.CenterCrop(image_size),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize([0.485, 0.456, 0.406],\n","                                                           [0.229, 0.224, 0.225])])"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"a1wqKdpPdF7W","executionInfo":{"status":"ok","timestamp":1681611546912,"user_tz":-210,"elapsed":39223,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Load Datas\n","train_set = datasets.CelebA(root='', download=download_dataset, split='train', target_type=[\"attr\", \"identity\"], transform=train_transforms)\n","test_set = datasets.CelebA(root='', download=download_dataset, split='test', target_type=[\"attr\", \"identity\"], transform=test_transforms)\n","valid_set = datasets.CelebA(root='', download=download_dataset, split='valid', target_type=[\"attr\", \"identity\"], transform=test_transforms)\n","\n","# DataLoader\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=workers)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=workers)\n","valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=workers)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Z1oMlpDcr8Yv","executionInfo":{"status":"ok","timestamp":1681611546913,"user_tz":-210,"elapsed":24,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Be0xLbL4tMKz","executionInfo":{"status":"ok","timestamp":1681611546914,"user_tz":-210,"elapsed":24,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Encoder Model\n","class Encoder(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Encoder, self).__init__()\n","        self.ngpu = ngpu\n","\n","        # input is nc x 64 x 64\n","        self.conv1 = nn.Conv2d(nc, nef, 4, 2, 1, bias=False)\n","        self.actv1 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef) x 32 x 32\n","        self.conv2 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n","        self.bnor2 = nn.BatchNorm2d(nef)\n","        self.actv2 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef) x 16 x 16\n","        self.conv3 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n","        self.bnor3 = nn.BatchNorm2d(nef)\n","        self.actv3 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef) x 8 x 8\n","        self.conv4 = nn.Conv2d(nef, nef * 2, 4, 2, 1, bias=False)\n","        self.bnor4 = nn.BatchNorm2d(nef * 2)\n","        self.actv4 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef*2) x 4 x 4\n","        # shaping would be here: nef*2 x 4 x 4 -> 2048\n","        # state size. 2048\n","        self.fllc5 = nn.Linear(nef*2*4*4, nef*1*4*4)\n","        self.actv5 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. 1024\n","\n","        # split features: 1024 -> 1022 + 2 \n","        # first classifier:\n","        self.fllc_main_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n","        self.actv_main_features1 = nn.LeakyReLU(0.2, inplace=True)\n","        self.dropout_main_features1 = nn.Dropout(p=0.5)\n","        self.fllc_main_features2 = nn.Linear(nef*1*4*4, nef*4)\n","        self.actv_main_features2 = nn.LeakyReLU(0.2, inplace=True)\n","        self.dropout_main_features2 = nn.Dropout(p=0.5)\n","        self.fllc_main_features3 = nn.Linear(nef*4, nef)\n","        self.actv_main_features3 = nn.LeakyReLU(0.2, inplace=True)\n","        self.fllc_main_features4 = nn.Linear(nef, 2)\n","        self.actv_main_features4 = nn.LogSoftmax(dim=1)\n","        # other features\n","        self.fllc_other_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n","        self.actv_other_features1 = nn.LeakyReLU(0.2, inplace=True)\n","        self.dropout_other_features1 = nn.Dropout(p=0.5)\n","        self.fllc_other_features2 = nn.Linear(nef*1*4*4, nef*1*4*4)\n","        self.actv_other_features2 = nn.LeakyReLU(0.2, inplace=True)\n","        self.dropout_other_features2 = nn.Dropout(p=0.5)\n","        self.fllc_other_features3 = nn.Linear(nef*1*4*4, nef*1*4*4 - 2)\n","        self.actv_other_features3 = nn.LeakyReLU(0.2, inplace=True)\n","        # aggregate features for output\n","\n","    def forward(self, x):\n","        # Part 1:\n","        x = self.conv1(x)\n","        x = self.actv1(x)\n","        x = self.conv2(x)\n","        x = self.bnor2(x)\n","        x = self.actv2(x)\n","        x = self.conv3(x)\n","        x = self.bnor3(x)\n","        x = self.actv3(x)\n","        x = self.conv4(x)\n","        x = self.bnor4(x)\n","        x = self.actv4(x)\n","        # flatten\n","        x = torch.flatten(x, start_dim = 1)\n","        # Part 2:\n","        x = self.fllc5(x)\n","        x = self.actv5(x)\n","        # first classifier:\n","        y1 = self.fllc_main_features1(x)\n","        y1 = self.actv_main_features1(y1)\n","        y1 = self.dropout_main_features1(y1)\n","        y1 = self.fllc_main_features2(y1)\n","        y1 = self.actv_main_features2(y1)\n","        y1 = self.dropout_main_features2(y1)\n","        y1 = self.fllc_main_features3(y1)\n","        y1 = self.actv_main_features3(y1)\n","        y1 = self.fllc_main_features4(y1)\n","        y1 = self.actv_main_features4(y1)\n","        # other features\n","        y3 = self.fllc_other_features1(x)\n","        y3 = self.actv_other_features1(y3)\n","        y3 = self.dropout_other_features1(y3)\n","        y3 = self.fllc_other_features2(y3)\n","        y3 = self.actv_other_features2(y3)\n","        y3 = self.dropout_other_features2(y3)\n","        y3 = self.fllc_other_features3(y3)\n","        y3 = self.actv_other_features3(y3)\n","        return y1, y3"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"JiMBC0CHtNX8","executionInfo":{"status":"ok","timestamp":1681611546914,"user_tz":-210,"elapsed":23,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Decoder Model\n","class Decoder(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Decoder, self).__init__()\n","        self.ngpu = ngpu\n","\n","        # input size is 1024\n","        self.fllc6 = nn.Linear(nef*1*4*4, ndf*2*4*4)\n","        self.actv6 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. 2048\n","        # shaping would be here: 2048 -> ndf*2 x 4 x 4\n","        # state size. (ndf*2) x 4 x 4\n","        self.cnvt7 = nn.ConvTranspose2d( ndf*2, ndf, 4, 2, 1, bias=False)\n","        self.bnor7 = nn.BatchNorm2d(ndf)\n","        self.actv7 = nn.ReLU(True)\n","        # state size. (ndf) x 8 x 8\n","        self.cnvt8 = nn.ConvTranspose2d(ndf, ndf, 4, 2, 1, bias=False)\n","        self.bnor8 = nn.BatchNorm2d(ndf)\n","        self.actv8 = nn.ReLU(True)\n","        # state size. (ndf) x 16 x 16\n","        self.cnvt9 = nn.ConvTranspose2d( ndf, ndf, 4, 2, 1, bias=False)\n","        self.bnor9 = nn.BatchNorm2d(ndf)\n","        self.actv9 = nn.ReLU(True)\n","        # state size. (ndf) x 32 x 32\n","        self.cnvt10 = nn.ConvTranspose2d( ndf, nc, 4, 2, 1, bias=False)\n","        self.actv10 = nn.Sigmoid()\n","        # state size. (nc) x 64 x 64\n","\n","    def forward(self, x):\n","        x = self.fllc6(x)\n","        x = self.actv6(x)\n","        x = x.view(batch_size, ndf*2, 4, 4)\n","        x = self.cnvt7(x)\n","        x = self.bnor7(x)\n","        x = self.actv7(x)\n","        x = self.cnvt8(x)\n","        x = self.bnor8(x)\n","        x = self.actv8(x)\n","        x = self.cnvt9(x)\n","        x = self.bnor9(x)\n","        x = self.actv9(x)\n","        x = self.cnvt10(x)\n","        x = self.actv10(x)\n","        return x"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2oNiIb7r98jC","executionInfo":{"status":"ok","timestamp":1681611546915,"user_tz":-210,"elapsed":24,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# AE Model + Noise\n","class AEModel(nn.Module):\n","    def __init__(self, ngpu, mode='train', miu=0, coef_for_var=0, g_eff_val=-3000):\n","        super(AEModel, self).__init__()\n","        self.ngpu = ngpu\n","        self.g_eff_val = g_eff_val\n","        self.miu = miu\n","        self.coef_for_var = coef_for_var\n","        self.mode = mode\n","        self.encoder = Encoder(ngpu).to(device)\n","        self.decoder = Decoder(ngpu).to(device)\n","\n","    def tune_noise(self, miu=0, coef_for_var=0, g_eff_val=-3000):\n","        self.miu = miu\n","        self.coef_for_var = coef_for_var\n","        self.g_eff_val = g_eff_val\n","\n","    def change_mode(self, mode='train'):\n","        self.mode = mode\n","\n","    def add_noise(self, nodes):\n","      with torch.no_grad():\n","        var = (self.coef_for_var) * (torch.mean(nodes).item())\n","        noise = self.miu + (var) * torch.randn(nodes.size())\n","        noise = noise.to(device)\n","        nodes.add_(noise)\n","        return nodes\n","\n","    def change_lbl(self, nodes, lbls):\n","      with torch.no_grad():\n","        lbls[lbls == 0] = self.g_eff_val\n","        lbls[lbls == 1] = 0\n","        nodes = lbls\n","        return nodes\n","\n","    def forward(self, x, y1_real_lbl=[]):\n","        if create_random_pics:\n","            yy = []\n","            yy = self.change_lbl(yy, y1_real_lbl)\n","            y = torch.cat((yy, x), 1)\n","            pic = self.decoder(y)\n","            return pic, yy\n","        else:\n","            y1, y3 = self.encoder(x)\n","            if self.mode=='use':\n","                if use_g:\n","                  y1 = self.change_lbl(y1, y1_real_lbl)\n","                y3 = self.add_noise(y3)\n","            y = torch.cat((y1, y3), 1)\n","            x = self.decoder(y)\n","            return x, y1, y3"]},{"cell_type":"code","source":["# Utilizer Model \n","class UtlModel(nn.Module):\n","    def __init__(self, ngpu):\n","        super(UtlModel, self).__init__()\n","        self.ngpu = ngpu\n","        \n","        # input is nc x 64 x 64 \n","        self.conv1 = nn.Conv2d(nc, nef, 4, 2, 1, bias=False)\n","        self.actv1 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef) x 32 x 32\n","        self.conv2 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n","        self.bnor2 = nn.BatchNorm2d(nef)\n","        self.actv2 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef) x 16 x 16\n","        self.conv3 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n","        self.bnor3 = nn.BatchNorm2d(nef)\n","        self.actv3 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef) x 8 x 8\n","        self.conv4 = nn.Conv2d(nef, nef * 2, 4, 2, 1, bias=False)\n","        self.bnor4 = nn.BatchNorm2d(nef * 2)\n","        self.actv4 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (nef*2) x 4 x 4\n","        # shaping would be here: nef*2 x 4 x 4 -> 2048\n","        # state size. 2048\n","        self.fllc5 = nn.Linear(nef*2*4*4, nef*1*4*4)\n","        self.actv5 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. 1024\n","\n","        # classifier:\n","        self.fllc_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n","        self.actv_features1 = nn.LeakyReLU(0.2, inplace=True)\n","        self.dropout_features1 = nn.Dropout(p=0.5)\n","        self.fllc_features2 = nn.Linear(nef*1*4*4, nef*4)\n","        self.actv_features2 = nn.LeakyReLU(0.2, inplace=True)\n","        self.dropout_features2 = nn.Dropout(p=0.5)\n","        self.fllc_features3 = nn.Linear(nef*4, nef)\n","        self.actv_features3 = nn.LeakyReLU(0.2, inplace=True)\n","        self.fllc_features4 = nn.Linear(nef, 2)\n","        self.actv_features4 = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x):\n","        # Part 1:\n","        x = self.conv1(x)\n","        x = self.actv1(x)\n","        x = self.conv2(x)\n","        x = self.bnor2(x)\n","        x = self.actv2(x)\n","        x = self.conv3(x)\n","        x = self.bnor3(x)\n","        x = self.actv3(x)\n","        x = self.conv4(x)\n","        x = self.bnor4(x)\n","        x = self.actv4(x)\n","        # flatten\n","        x = torch.flatten(x, start_dim = 1)\n","        # Part 2:\n","        x = self.fllc5(x)\n","        x = self.actv5(x)\n","        # classifier: \n","        y1 = self.fllc_features1(x)\n","        y1 = self.actv_features1(y1)\n","        y1 = self.dropout_features1(y1)\n","        y1 = self.fllc_features2(y1)\n","        y1 = self.actv_features2(y1)\n","        y1 = self.dropout_features2(y1)\n","        y1 = self.fllc_features3(y1)\n","        y1 = self.actv_features3(y1)\n","        y1 = self.fllc_features4(y1)\n","        y1 = self.actv_features4(y1)\n","        return y1"],"metadata":{"id":"hv_R7Yujy3A1","executionInfo":{"status":"ok","timestamp":1681611546915,"user_tz":-210,"elapsed":23,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"vZkZ6c_stXg7","executionInfo":{"status":"ok","timestamp":1681611552417,"user_tz":-210,"elapsed":5525,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Create the networks\n","netAE = AEModel(ngpu).to(device)\n","utilizerModel = UtlModel(ngpu).to(device)\n","\n","# Handle multi-gpu if desired\n","if (device.type == 'cuda') and (ngpu > 1):\n","    netAE = nn.DataParallel(netAE, list(range(ngpu)))\n","    utilizerModel = nn.DataParallel(utilizerModel, list(range(ngpu)))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"77ovq9TiwQ-S","executionInfo":{"status":"ok","timestamp":1681611552417,"user_tz":-210,"elapsed":15,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Function - Load:\n","def load_model(saving_path, name, number, model, device):\n","\n","  checkpoint = torch.load(saving_path + 'checkpoint-' + name + '-' + str(number) + '.pth', map_location=device)\n","  res = checkpoint['res']\n","  model.load_state_dict(checkpoint['state_dict'])\n","  return {'model':model,\n","          'res':res}"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"8-YMhj_Pwsyo","executionInfo":{"status":"ok","timestamp":1681611554255,"user_tz":-210,"elapsed":1852,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["# Load Last Checkpoint:\n","ae_load = load_model(p2r_model_path, 'ae', p2r_model_number, netAE, device)\n","utl_load = load_model(utl_model_path, 'ins', utl_model_number, utilizerModel, device)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"8DTOaniHuWF2","executionInfo":{"status":"ok","timestamp":1681611554256,"user_tz":-210,"elapsed":11,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"outputs":[],"source":["def extract_targets(labels):\n","    male_labels =  labels[0][:, using_index]\n","    female_labels = torch.add(1, -male_labels)\n","    gender_target = torch.cat((female_labels.view(batch_size,1), male_labels.view(batch_size,1)), 1).float() \n","    gender_target = gender_target.to(device)\n","    return gender_target"]},{"cell_type":"code","source":["# Permutation case test part1:\n","create_random_pics = False\n","netAE.change_mode('use')\n","netAE.eval()\n","\n","middle_layer = torch.empty((1,1022), dtype=torch.float32)\n","middle_layer = middle_layer.to(device)\n","new_loader = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=workers)\n","prog_bar = tqdm(enumerate(new_loader), total=len(new_loader))\n","with torch.no_grad():\n","  for i, data in prog_bar:\n","    inputs, labels = data[0], data[1]\n","    inputs = inputs.to(device)\n","    gender_target = extract_targets(labels)\n","    output, y1, y3 = netAE.forward(inputs, gender_target)\n","    middle_layer = torch.cat([middle_layer, y3])"],"metadata":{"id":"W5NDmDJFy6j4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Permutation case test part2:\n","r=torch.randperm(middle_layer.shape[0])\n","middle_layer2 = middle_layer[r][:]"],"metadata":{"id":"8MUK-M184khh","executionInfo":{"status":"ok","timestamp":1681611671895,"user_tz":-210,"elapsed":20,"user":{"displayName":"jam thesis","userId":"13286242041191639337"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Permutation case test part3:\n","# create pics and calc acc: case2 (c2) - smile = true\n","create_random_pics = True\n","use_g = True\n","netAE.tune_noise(miu, coef_for_var, g_eff_val=-3000)\n","c2_utl_acc = []\n","x10 = []\n","y10 = []\n","round = middle_layer.shape[0]\n","for i in range(round):\n","  p_attr = middle_layer2[i][:]\n","  p_attr = p_attr.reshape(torch.Size([1, 1022]))\n","  n_attr = (torch.Tensor([[1,0]])).to(device)\n","  netAE.change_mode('use')\n","  netAE.eval()\n","  output, label = netAE.forward(p_attr, n_attr)\n","  utl_out1 = utilizerModel(output)\n","  ps_utl1 = torch.exp(utl_out1)\n","  x10.append(ps_utl1[0][0].item())\n","  y10.append(ps_utl1[0][1].item())\n","  if (ps_utl1[0][0].item() >= ps_utl1[0][1].item()):\n","    c = 1\n","  else:\n","    c = 0\n","  c2_utl_acc.append(c)\n","c2_acc1 = sum(c2_utl_acc) / round\n","print(f\"Accuracy of utilizer for smiling: {c2_acc1:.4f}\")"],"metadata":{"id":"Jw241HxZ5Eqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Permutation case test part4:\n","# create pics and calc acc: case2 (c2) - smile = false\n","create_random_pics = True\n","use_g = True\n","netAE.tune_noise(miu, coef_for_var, g_eff_val=-3000)\n","c2_utl_acc2 = []\n","x01 = []\n","y01 = []\n","round = middle_layer.shape[0]\n","for i in range(round):\n","  p_attr = middle_layer2[i][:]\n","  p_attr = p_attr.reshape(torch.Size([1, 1022]))\n","  n_attr = (torch.Tensor([[0,1]])).to(device)\n","  netAE.change_mode('use')\n","  netAE.eval()\n","  output, label = netAE.forward(p_attr, n_attr)\n","  utl_out1 = utilizerModel(output)\n","  ps_utl1 = torch.exp(utl_out1)\n","  x01.append(ps_utl1[0][0].item())\n","  y01.append(ps_utl1[0][1].item())\n","  if (ps_utl1[0][0].item() < ps_utl1[0][1].item()):\n","    c = 1\n","  else:\n","    c = 0\n","  c2_utl_acc2.append(c)\n","c2_acc2 = sum(c2_utl_acc2) / round\n","print(f\"Accuracy of utilizer for not smiling: {c2_acc2:.4f}\")"],"metadata":{"id":"qc_9NGWBRi7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'svg'\n","matplotlib.style.use('default')\n","from matplotlib import rc\n","\n","rc('font', **{'family':'serif','serif':['Palatino']})\n","rc('text', usetex=True)\n","fig = plt.figure(figsize=(5,3))\n","matplotlib.rcParams.update({'font.size': 10})\n","fig.tight_layout()\n","\n","plt.hist(x01, 50, color='blue', label='Smiling')\n","plt.hist(x10, 50, color='green', label='Not Smiling')\n","\n","plt.xlabel(\"Output of Utility Provider\")\n","plt.ylabel(\"Number of Occurrence\")\n","plt.grid(color = 'gray', linestyle = 'dotted', linewidth = 0.5)\n","plt.legend(frameon=True)\n","plt.savefig(saving_path + args.result_file_name + \".svg\", bbox_inches = 'tight')\n","plt.savefig(saving_path + args.result_file_name + \".png\", bbox_inches = 'tight')\n","plt.savefig(saving_path + args.result_file_name + \".eps\", bbox_inches = 'tight', format='eps')\n","plt.show()"],"metadata":{"id":"rjpMONAb6PvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Permutation case test part5: Create random faces with controlled utilizer label\n","create_random_pics = True\n","use_g = True\n","netAE.tune_noise(miu, coef_for_var, g_eff_val=-3000)\n","p_attr = middle_layer2[3][:]\n","p_attr = p_attr.reshape(torch.Size([1, 1022]))\n","n_attr1 = (torch.Tensor([[1, 0]])).to(device)\n","n_attr2 = (torch.Tensor([[0, 1]])).to(device)\n","netAE.change_mode('use')\n","netAE.eval()\n","output1, label1 = netAE.forward(p_attr, n_attr1)\n","output2, label2 = netAE.forward(p_attr, n_attr2)\n","\n","plt.figure(figsize=(2,2))\n","plt.axis(\"off\")\n","plt.title(\"Images\")\n","plt.imshow(np.transpose(vutils.make_grid(output1, padding=2, normalize=True).cpu(),(1,2,0)))\n","\n","plt.figure(figsize=(2,2))\n","plt.axis(\"off\")\n","plt.title(\"Images\")\n","plt.imshow(np.transpose(vutils.make_grid(output2, padding=2, normalize=True).cpu(),(1,2,0)))"],"metadata":{"id":"B57XyL7w6XhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Create random faces with controlled utilizer label\n","\n","create_random_pics = True\n","use_g = True\n","netAE.tune_noise(miu, coef_for_var, g_eff_val=-3000)\n","selected_indices = [4, 100, 212, 26, 85, 158, 821, 400]\n","o1 = torch.empty((0, 3, 64, 64), dtype=torch.float32)\n","o1 = o1.to(device)\n","o2 = torch.empty((0, 3, 64, 64), dtype=torch.float32)\n","o2 = o2.to(device)\n","\n","for i in selected_indices:\n","  p_attr = middle_layer2[i][:]\n","  p_attr = p_attr.reshape(torch.Size([1, 1022]))\n","  n_attr1 = (torch.Tensor([[1, 0]])).to(device)\n","  n_attr2 = (torch.Tensor([[0, 1]])).to(device)\n","  netAE.change_mode('use')\n","  netAE.eval()\n","  output1, label1 = netAE.forward(p_attr, n_attr1)\n","  output2, label2 = netAE.forward(p_attr, n_attr2)\n","  o1 = torch.cat([o1, output1])\n","  o2 = torch.cat([o2, output2])\n","\n","plt.figure(figsize=(16,16))\n","plt.axis(\"off\")\n","plt.imshow(np.transpose(vutils.make_grid(o1.to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n","plt.savefig(saving_path + \"modified_pics_by_only_g10.png\")\n","plt.figure(figsize=(16,16))\n","plt.axis(\"off\")\n","plt.imshow(np.transpose(vutils.make_grid(o2.to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n","plt.savefig(saving_path + \"modified_pics_by_only_g01.png\")"],"metadata":{"id":"znC3PD97SQs3"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Ix-cqqnj2gtB0Gtbk8fxylF4sme51qBS","timestamp":1675465733605},{"file_id":"16ETSjS9xKtubP5rfeGGx58PfGXnk0FoJ","timestamp":1670450260200}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}