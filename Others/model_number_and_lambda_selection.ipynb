{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDshGMkxWQFV"
   },
   "outputs": [],
   "source": [
    "# Adjustable Privacy - model_number_and_lambda_selection.ipynb\n",
    "# - Uses image datasets (CelebA).\n",
    "# - This code obfuscate test set by all model numbers (of obfuscator) without g and f; then evaluate two trained machine on them (one infer private attribute (gender) and another infer utility attribute)\n",
    "# - reports the result of impact of choosing model number by ploting (saves result and plot)\n",
    "# - Also after selection of a model, obfuscate testset by this model considering g and by different lambdas and evaluate two mentioned machine on them.\n",
    "# - reports the result of impact of choosing lambda by ploting (saves result and plot)\n",
    "# - It can load previously saved results.\n",
    "# - You can manage notebook parameters in parser block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8qQgkY0Whqt"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "import random\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from math import floor\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt install cm-super dvipng texlive-latex-extra texlive-latex-recommended texlive texlive-fonts-recommended\n",
    "import os\n",
    "from matplotlib.pyplot import text\n",
    "matplotlib.style.use('default')\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('font', **{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)"
   ],
   "metadata": {
    "id": "9jrMeUwH6ViH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Parser\n",
    "parser = argparse.ArgumentParser(description='Adjustable Privacy '\n",
    "                                 + 'Uses image dataset (CelebA). '\n",
    "                                 + 'This code obfuscate test set by all model numbers (of obfuscator) without g and f; then evaluate two trained machine on them (one infer private attribute (gender) and another infer utility attribute). '\n",
    "                                 + 'reports the result of impact of choosing model number by plotting (saves result and plot) '\n",
    "                                 + 'Also after selection of a model, obfuscate testset by this model considering g and by different lambdas and evaluate two mentioned machine on them. '\n",
    "                                 + 'reports the result of impact of choosing lambda by plotting (saves result and plot) '\n",
    "                                 + 'It can load previously saved results. ')\n",
    "\n",
    "parser.add_argument('--obf_model_number', type=int, required=True, help = 'The epoch number of desired obfuscator model (model number)')\n",
    "parser.add_argument('--utl_model_number', type=int, required=True, help = 'The epoch number of desired trained model (infer target label - smiling, open mouth, and highcheekbone)')\n",
    "parser.add_argument('--adv_model_number', type=int, required=True, help = 'The epoch number of desired trained model (infer gender)')\n",
    "parser.add_argument('--target_index', type=int, required=True, help = 'smiling(31), open mouth(21), and high cheekbone(19)')\n",
    "parser.add_argument('--obf_load_path', type=str, required=True, help = 'Full path on your google drive to load obfuscator model from. Like \"drive/MyDrive/adjustable-privacy/Models/CelebA-G-S-Obfuscator/\"')\n",
    "parser.add_argument('--utl_load_path', type=str, required=True, help = 'Full path on your google drive to load trained model (infer target label - smiling, open mouth, and high cheekbone) from. Like \"drive/MyDrive/adjustable-privacy/Models/CelebA-S/\"')\n",
    "parser.add_argument('--adv_load_path', type=str, required=True, help = 'Full path on your google drive to load trained model (infer gender) from. Like \"drive/MyDrive/adjustable-privacy/Models/CelebA-G/\"')\n",
    "parser.add_argument('--save_path', type=str, required=True, help = 'Full path on your google drive to save results. Like \"drive/MyDrive/adjustable-privacy/Others/Results/\"')\n",
    "parser.add_argument('--download_dataset', default = False, help = 'Accepts \"True\" or \"False\". Download CelebA dataset or use CelebA shared directory.')\n",
    "parser.add_argument('--dataset_path', type=str, default = \"\", help = '(if --download_dataset=False) Full path on your google drive to CelebA shared directory shortcut. Like \"drive/MyDrive/adjustable-privacy/Datasets/CelebA/\"')\n",
    "parser.add_argument('--load_result_model_number', default = False, help = 'Accepts \"True\" or \"False\". load last saved result of model number experiment')\n",
    "parser.add_argument('--last_saved_model_number', type=int, default = 200, help = '(In model number experiment) In case of resuming experiment use last saved number and in case of loading result, set to desired number.')\n",
    "parser.add_argument('--whole_model_numbers', type=int, default = 200, help = '(In model number experiment) Number of whole model numbers. ')\n",
    "parser.add_argument('--model_number_result_file_name', type=str, default = \"\", help = 'a short name specify model number experiment result like:\"GS-modelnumber\" ')\n",
    "parser.add_argument('--load_result_lambda', default = False, help = 'Accepts \"True\" or \"False\". load last saved result of lambda experiment')\n",
    "parser.add_argument('--lambda_result_file_name', type=str, default = \"\", help = 'a short name specify result of lambda experiment like:\"GS-lambda-obf183\" or \"\"GS-lambda-obf13\"\"')\n",
    "\n",
    "command_string = \"--obf_model_number 183\" \\\n",
    "\" --utl_model_number 6\" \\\n",
    "\" --adv_model_number 9\" \\\n",
    "\" --target_index 20\" \\\n",
    "\" --obf_load_path drive/MyDrive/adjustable-privacy/Models/CelebA-G-S-Obfuscator/\" \\\n",
    "\" --utl_load_path drive/MyDrive/adjustable-privacy/Models/CelebA-S/\" \\\n",
    "\" --adv_load_path drive/MyDrive/adjustable-privacy/Models/CelebA-G/\" \\\n",
    "\" --save_path drive/MyDrive/adjustable-privacy/Others/Results/\" \\\n",
    "\" --download_dataset False\" \\\n",
    "\" --dataset_path drive/MyDrive/adjustable-privacy/Datasets/CelebA/\" \\\n",
    "\" --load_result_model_number True\" \\\n",
    "\" --last_saved_model_number 200\" \\\n",
    "\" --whole_model_numbers 200\" \\\n",
    "\" --model_number_result_file_name GS-modelnumber\" \\\n",
    "\" --load_result_lambda True\" \\\n",
    "\" --lambda_result_file_name GS-lambda-obf183\"\n",
    "\n",
    "args = parser.parse_args(command_string.split())"
   ],
   "metadata": {
    "id": "3F1uUYNBNHNm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz5QbqGAW8T6"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters:\n",
    "manual_seed = 20\n",
    "image_size = 64\n",
    "use_whole_dataset = True\n",
    "usage_percent = 1.00\n",
    "celeba_male_index = 20\n",
    "celeba_smiling_index = 31\n",
    "celeba_mouth_open_index = 21\n",
    "celeba_high_cheekbone_index = 19\n",
    "using_index = args.target_index\n",
    "adv_using_index = celeba_male_index\n",
    "batch_size = 64 #64\n",
    "\n",
    "files_not_ready = True\n",
    "download_dataset = args.download_dataset=='True'\n",
    "dataset_folder_path = args.dataset_path\n",
    "data_dir = 'celeba'\n",
    "saving_path = args.save_path\n",
    "\n",
    "# creating modified dataset:\n",
    "m_version = 'model-number-and-lambda-experiments'\n",
    "on_fly_modified_dataset_saving_path = 'modifiedDatasets/' + m_version + '/'\n",
    "\n",
    "p2r_model_number = args.obf_model_number\n",
    "p2r_model_path = args.obf_load_path\n",
    "utl_model_number = args.utl_model_number\n",
    "utl_model_path = args.utl_load_path\n",
    "adv_model_number = args.adv_model_number\n",
    "adv_model_path = args.adv_load_path\n",
    "\n",
    "use_g = True\n",
    "g_eff_val = -10000\n",
    "miu = 0\n",
    "coef_for_var = 0\n",
    "\n",
    "suffling_main_train_data = False\n",
    "suffling_modified_train_data = False\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "# Size of feature maps in encoder\n",
    "nef = 64\n",
    "# Size of feature maps in decoder\n",
    "ndf = 64\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3VlZ8lvZIms"
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6M5FAIqZQcK"
   },
   "outputs": [],
   "source": [
    "# Mount google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "632eDVH1ZJG7"
   },
   "outputs": [],
   "source": [
    "# getting dataset ready from shared directory\n",
    "if not download_dataset:\n",
    "    dataset_zip_path = dataset_folder_path + '/Img/img_align_celeba.zip'\n",
    "    list_eval_partition_path = dataset_folder_path + '/Eval/list_eval_partition.txt'\n",
    "    identity_celeba_path = dataset_folder_path + '/Anno/identity_CelebA.txt'\n",
    "    list_attr_celeba_path = dataset_folder_path + '/Anno/list_attr_celeba.txt'\n",
    "    list_bbox_celeba_path = dataset_folder_path + '/Anno/list_bbox_celeba.txt'\n",
    "    list_landmarks_align_celeba_path = dataset_folder_path + '/Anno/list_landmarks_align_celeba.txt'\n",
    "\n",
    "    try:\n",
    "      os.mkdir(data_dir)\n",
    "      print(\"data folder created successfully\")\n",
    "    except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))\n",
    "\n",
    "    shutil.copyfile(dataset_zip_path, data_dir + r'/img_align_celeba.zip')\n",
    "    shutil.copyfile(list_eval_partition_path, data_dir + r'/list_eval_partition.txt')\n",
    "    shutil.copyfile(identity_celeba_path, data_dir + r'/identity_CelebA.txt')\n",
    "    shutil.copyfile(list_attr_celeba_path, data_dir + r'/list_attr_celeba.txt')\n",
    "    shutil.copyfile(list_bbox_celeba_path, data_dir + r'/list_bbox_celeba.txt')\n",
    "    shutil.copyfile(list_landmarks_align_celeba_path, data_dir + r'/list_landmarks_align_celeba.txt')\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(data_dir + r'/img_align_celeba')\n",
    "        print(\"old unzipped directory removed successfully\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s\" % (e.strerror))\n",
    "\n",
    "    archive = data_dir + r'/img_align_celeba.zip'\n",
    "    with ZipFile(archive, 'r') as zip:\n",
    "       zip.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# make saving path dir\n",
    "try:\n",
    "    os.makedirs(saving_path)\n",
    "    print(\"saving_path directory created successfully\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s\" % (e.strerror))"
   ],
   "metadata": {
    "id": "LzwCI40IhliK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function - use some percent of data\n",
    "def shorten_dataset(dataset, usage_percent=1.0):\n",
    "  len_used = floor(len(dataset)*usage_percent)\n",
    "  len_not_used = len(dataset) - len_used\n",
    "  used_dataset, not_used_dataset = random_split(dataset, [len_used, len_not_used], generator=torch.Generator().manual_seed(manual_seed))\n",
    "  return used_dataset"
   ],
   "metadata": {
    "id": "abeuI8cYQ2kU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ0HTan5cWmA"
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "test_transforms = transforms.Compose([transforms.Resize(image_size),\n",
    "                                      transforms.CenterCrop(image_size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1wqKdpPdF7W"
   },
   "outputs": [],
   "source": [
    "# Load Datas\n",
    "test_set = datasets.CelebA(root='', download=download_dataset, split='test', target_type=[\"attr\", \"identity\"], transform=test_transforms)\n",
    "# shorten Dataset\n",
    "if not use_whole_dataset:\n",
    "  test_set = shorten_dataset(test_set, usage_percent)\n",
    "# DataLoader\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1oMlpDcr8Yv"
   },
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be0xLbL4tMKz"
   },
   "outputs": [],
   "source": [
    "# Encoder Model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # input is nc x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(nc, nef, 4, 2, 1, bias=False)\n",
    "        self.actv1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor2 = nn.BatchNorm2d(nef)\n",
    "        self.actv2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor3 = nn.BatchNorm2d(nef)\n",
    "        self.actv3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(nef, nef * 2, 4, 2, 1, bias=False)\n",
    "        self.bnor4 = nn.BatchNorm2d(nef * 2)\n",
    "        self.actv4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef*2) x 4 x 4\n",
    "        # shaping would be here: nef*2 x 4 x 4 -> 2048\n",
    "        # state size. 2048\n",
    "        self.fllc5 = nn.Linear(nef*2*4*4, nef*1*4*4)\n",
    "        self.actv5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. 1024\n",
    "\n",
    "        # split features: 1024 -> 1022 + 2\n",
    "        # first classifier:\n",
    "        self.fllc_main_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_main_features1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_main_features1 = nn.Dropout(p=0.5)\n",
    "        self.fllc_main_features2 = nn.Linear(nef*1*4*4, nef*4)\n",
    "        self.actv_main_features2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_main_features2 = nn.Dropout(p=0.5)\n",
    "        self.fllc_main_features3 = nn.Linear(nef*4, nef)\n",
    "        self.actv_main_features3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.fllc_main_features4 = nn.Linear(nef, 2)\n",
    "        self.actv_main_features4 = nn.LogSoftmax(dim=1)\n",
    "        # other features\n",
    "        self.fllc_other_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_other_features1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_other_features1 = nn.Dropout(p=0.5)\n",
    "        self.fllc_other_features2 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_other_features2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_other_features2 = nn.Dropout(p=0.5)\n",
    "        self.fllc_other_features3 = nn.Linear(nef*1*4*4, nef*1*4*4 - 2)\n",
    "        self.actv_other_features3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # aggregate features for output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        x = self.conv1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnor2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bnor3(x)\n",
    "        x = self.actv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bnor4(x)\n",
    "        x = self.actv4(x)\n",
    "        # flatten\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        # Part 2:\n",
    "        x = self.fllc5(x)\n",
    "        x = self.actv5(x)\n",
    "        # first classifier:\n",
    "        y1 = self.fllc_main_features1(x)\n",
    "        y1 = self.actv_main_features1(y1)\n",
    "        y1 = self.dropout_main_features1(y1)\n",
    "        y1 = self.fllc_main_features2(y1)\n",
    "        y1 = self.actv_main_features2(y1)\n",
    "        y1 = self.dropout_main_features2(y1)\n",
    "        y1 = self.fllc_main_features3(y1)\n",
    "        y1 = self.actv_main_features3(y1)\n",
    "        y1 = self.fllc_main_features4(y1)\n",
    "        y1 = self.actv_main_features4(y1)\n",
    "        # other features\n",
    "        y3 = self.fllc_other_features1(x)\n",
    "        y3 = self.actv_other_features1(y3)\n",
    "        y3 = self.dropout_other_features1(y3)\n",
    "        y3 = self.fllc_other_features2(y3)\n",
    "        y3 = self.actv_other_features2(y3)\n",
    "        y3 = self.dropout_other_features2(y3)\n",
    "        y3 = self.fllc_other_features3(y3)\n",
    "        y3 = self.actv_other_features3(y3)\n",
    "        return y1, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiMBC0CHtNX8"
   },
   "outputs": [],
   "source": [
    "# Decoder Model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # input size is 1024\n",
    "        self.fllc6 = nn.Linear(nef*1*4*4, ndf*2*4*4)\n",
    "        self.actv6 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. 2048\n",
    "        # shaping would be here: 2048 -> ndf*2 x 4 x 4\n",
    "        # state size. (ndf*2) x 4 x 4\n",
    "        self.cnvt7 = nn.ConvTranspose2d( ndf*2, ndf, 4, 2, 1, bias=False)\n",
    "        self.bnor7 = nn.BatchNorm2d(ndf)\n",
    "        self.actv7 = nn.ReLU(True)\n",
    "        # state size. (ndf) x 8 x 8\n",
    "        self.cnvt8 = nn.ConvTranspose2d(ndf, ndf, 4, 2, 1, bias=False)\n",
    "        self.bnor8 = nn.BatchNorm2d(ndf)\n",
    "        self.actv8 = nn.ReLU(True)\n",
    "        # state size. (ndf) x 16 x 16\n",
    "        self.cnvt9 = nn.ConvTranspose2d( ndf, ndf, 4, 2, 1, bias=False)\n",
    "        self.bnor9 = nn.BatchNorm2d(ndf)\n",
    "        self.actv9 = nn.ReLU(True)\n",
    "        # state size. (ndf) x 32 x 32\n",
    "        self.cnvt10 = nn.ConvTranspose2d( ndf, nc, 4, 2, 1, bias=False)\n",
    "        self.actv10 = nn.Sigmoid()\n",
    "        # state size. (nc) x 64 x 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fllc6(x)\n",
    "        x = self.actv6(x)\n",
    "        x = x.view(batch_size, ndf*2, 4, 4)\n",
    "        x = self.cnvt7(x)\n",
    "        x = self.bnor7(x)\n",
    "        x = self.actv7(x)\n",
    "        x = self.cnvt8(x)\n",
    "        x = self.bnor8(x)\n",
    "        x = self.actv8(x)\n",
    "        x = self.cnvt9(x)\n",
    "        x = self.bnor9(x)\n",
    "        x = self.actv9(x)\n",
    "        x = self.cnvt10(x)\n",
    "        x = self.actv10(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oNiIb7r98jC"
   },
   "outputs": [],
   "source": [
    "# AE Model + Noise\n",
    "class AEModel(nn.Module):\n",
    "    def __init__(self, ngpu, mode='train', miu=0, coef_for_var=0, g_eff_val=-10000):\n",
    "        super(AEModel, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.g_eff_val = g_eff_val\n",
    "        self.miu = miu\n",
    "        self.coef_for_var = coef_for_var\n",
    "        self.mode = mode\n",
    "        self.encoder = Encoder(ngpu).to(device)\n",
    "        self.decoder = Decoder(ngpu).to(device)\n",
    "\n",
    "    def tune_noise(self, miu=0, coef_for_var=0, g_eff_val=-10000):\n",
    "        self.miu = miu\n",
    "        self.coef_for_var = coef_for_var\n",
    "        self.g_eff_val = g_eff_val\n",
    "\n",
    "    def change_mode(self, mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "    def add_noise(self, nodes):\n",
    "      with torch.no_grad():\n",
    "        var = (self.coef_for_var) * (torch.mean(nodes).item())\n",
    "        noise = self.miu + (var) * torch.randn(nodes.size())\n",
    "        noise = noise.to(device)\n",
    "        nodes.add_(noise)\n",
    "        return nodes\n",
    "\n",
    "    def change_lbl(self, nodes, lbls):\n",
    "      with torch.no_grad():\n",
    "        lbls[lbls == 0] = self.g_eff_val\n",
    "        lbls[lbls == 1] = 0\n",
    "        nodes = lbls\n",
    "        return nodes\n",
    "\n",
    "    def forward(self, x, y1_real_lbl=[]):\n",
    "        y1, y3 = self.encoder(x)\n",
    "        if self.mode=='use':\n",
    "            if use_g:\n",
    "              y1 = self.change_lbl(y1, y1_real_lbl)\n",
    "            y3 = self.add_noise(y3)\n",
    "        y = torch.cat((y1, y3), 1)\n",
    "        x = self.decoder(y)\n",
    "        return x, y1"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Utilizer Model\n",
    "class UtlModel(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(UtlModel, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # input is nc x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(nc, nef, 4, 2, 1, bias=False)\n",
    "        self.actv1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor2 = nn.BatchNorm2d(nef)\n",
    "        self.actv2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor3 = nn.BatchNorm2d(nef)\n",
    "        self.actv3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(nef, nef * 2, 4, 2, 1, bias=False)\n",
    "        self.bnor4 = nn.BatchNorm2d(nef * 2)\n",
    "        self.actv4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef*2) x 4 x 4\n",
    "        # shaping would be here: nef*2 x 4 x 4 -> 2048\n",
    "        # state size. 2048\n",
    "        self.fllc5 = nn.Linear(nef*2*4*4, nef*1*4*4)\n",
    "        self.actv5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. 1024\n",
    "\n",
    "        # classifier:\n",
    "        self.fllc_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_features1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_features1 = nn.Dropout(p=0.5)\n",
    "        self.fllc_features2 = nn.Linear(nef*1*4*4, nef*4)\n",
    "        self.actv_features2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_features2 = nn.Dropout(p=0.5)\n",
    "        self.fllc_features3 = nn.Linear(nef*4, nef)\n",
    "        self.actv_features3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.fllc_features4 = nn.Linear(nef, 2)\n",
    "        self.actv_features4 = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        x = self.conv1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnor2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bnor3(x)\n",
    "        x = self.actv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bnor4(x)\n",
    "        x = self.actv4(x)\n",
    "        # flatten\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        # Part 2:\n",
    "        x = self.fllc5(x)\n",
    "        x = self.actv5(x)\n",
    "        # classifier: \n",
    "        y1 = self.fllc_features1(x)\n",
    "        y1 = self.actv_features1(y1)\n",
    "        y1 = self.dropout_features1(y1)\n",
    "        y1 = self.fllc_features2(y1)\n",
    "        y1 = self.actv_features2(y1)\n",
    "        y1 = self.dropout_features2(y1)\n",
    "        y1 = self.fllc_features3(y1)\n",
    "        y1 = self.actv_features3(y1)\n",
    "        y1 = self.fllc_features4(y1)\n",
    "        y1 = self.actv_features4(y1)\n",
    "        return y1"
   ],
   "metadata": {
    "id": "QfZoFAcZjjzq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# For Adversary\n",
    "# Adversary Model \n",
    "class AdvModel(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(AdvModel, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # input is nc x 64 x 64 \n",
    "        self.conv1 = nn.Conv2d(nc, nef, 4, 2, 1, bias=False)\n",
    "        self.actv1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor2 = nn.BatchNorm2d(nef)\n",
    "        self.actv2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(nef, nef, 4, 2, 1, bias=False)\n",
    "        self.bnor3 = nn.BatchNorm2d(nef)\n",
    "        self.actv3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(nef, nef * 2, 4, 2, 1, bias=False)\n",
    "        self.bnor4 = nn.BatchNorm2d(nef * 2)\n",
    "        self.actv4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. (nef*2) x 4 x 4\n",
    "        # shaping would be here: nef*2 x 4 x 4 -> 2048\n",
    "        # state size. 2048\n",
    "        self.fllc5 = nn.Linear(nef*2*4*4, nef*1*4*4)\n",
    "        self.actv5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # state size. 1024\n",
    "\n",
    "        # classifier: \n",
    "        self.fllc_features1 = nn.Linear(nef*1*4*4, nef*1*4*4)\n",
    "        self.actv_features1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_features1 = nn.Dropout(p=0.5)\n",
    "        self.fllc_features2 = nn.Linear(nef*1*4*4, nef*4)\n",
    "        self.actv_features2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.dropout_features2 = nn.Dropout(p=0.5)\n",
    "        self.fllc_features3 = nn.Linear(nef*4, nef)\n",
    "        self.actv_features3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.fllc_features4 = nn.Linear(nef, 2)\n",
    "        self.actv_features4 = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        x = self.conv1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnor2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bnor3(x)\n",
    "        x = self.actv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bnor4(x)\n",
    "        x = self.actv4(x)\n",
    "        # flatten\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        # Part 2:\n",
    "        x = self.fllc5(x)\n",
    "        x = self.actv5(x)\n",
    "        # classifier: \n",
    "        y1 = self.fllc_features1(x)\n",
    "        y1 = self.actv_features1(y1)\n",
    "        y1 = self.dropout_features1(y1)\n",
    "        y1 = self.fllc_features2(y1)\n",
    "        y1 = self.actv_features2(y1)\n",
    "        y1 = self.dropout_features2(y1)\n",
    "        y1 = self.fllc_features3(y1)\n",
    "        y1 = self.actv_features3(y1)\n",
    "        y1 = self.fllc_features4(y1)\n",
    "        y1 = self.actv_features4(y1)\n",
    "        return y1"
   ],
   "metadata": {
    "id": "4IIhO0uRj6LU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZkZ6c_stXg7"
   },
   "outputs": [],
   "source": [
    "# Create \n",
    "netAE = AEModel(ngpu).to(device)\n",
    "adversaryModel = AdvModel(ngpu).to(device)\n",
    "utilizerModel = UtlModel(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netAE = nn.DataParallel(netAE, list(range(ngpu)))\n",
    "    adversaryModel = nn.DataParallel(adversaryModel, list(range(ngpu)))\n",
    "    utilizerModel = nn.DataParallel(utilizerModel, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k7gGE5uwP27"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Function - Save:\n",
    "def save_data(saving_path, name, res):\n",
    "  checkpoint = {'res': res}\n",
    "  torch.save(checkpoint, saving_path + name + '.pth')\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data(saving_path, name):\n",
    "  checkpoint = torch.load(saving_path + name + '.pth', map_location=device)\n",
    "  res = checkpoint['res']\n",
    "  return res"
   ],
   "metadata": {
    "id": "_jizdsY9Njyg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77ovq9TiwQ-S"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Function - Load:\n",
    "def load_model(saving_path, name, number, model, device):\n",
    "\n",
    "  checkpoint = torch.load(saving_path + 'checkpoint-' + name + '-' + str(number) + '.pth', map_location=device)\n",
    "  res = checkpoint['res']\n",
    "  model.load_state_dict(checkpoint['state_dict'])\n",
    "  return {'model':model,\n",
    "          'res':res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-YMhj_Pwsyo"
   },
   "outputs": [],
   "source": [
    "# Load Last Checkpoint:\n",
    "\n",
    "def load_models():\n",
    "  ae_load = load_model(p2r_model_path, 'ae', p2r_model_number, netAE, device)\n",
    "  adv_load = load_model(adv_model_path, 'ins', adv_model_number, adversaryModel, device)\n",
    "  utl_load = load_model(utl_model_path, 'ins', utl_model_number, utilizerModel, device)\n",
    "  return ae_load, adv_load, utl_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DTOaniHuWF2"
   },
   "outputs": [],
   "source": [
    "def extract_targets(labels):\n",
    "    male_labels = labels[0][:, using_index]\n",
    "    female_labels = torch.add(1, -male_labels)\n",
    "    gender_target = torch.cat((female_labels.view(batch_size,1), male_labels.view(batch_size,1)), 1).float()\n",
    "    gender_target = gender_target.to(device)\n",
    "    return gender_target"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "m_celeba_dir = on_fly_modified_dataset_saving_path + 'celeba/'\n",
    "img_celeba_dir = on_fly_modified_dataset_saving_path + 'celeba/img_align_celeba/'"
   ],
   "metadata": {
    "id": "OuN8uPpnOE97"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create new dataset folder:\n",
    "def create_modified_dataset_folder():\n",
    "  try:\n",
    "      os.mkdir('modifiedDatasets')\n",
    "  except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))\n",
    "  try:\n",
    "      os.mkdir(on_fly_modified_dataset_saving_path)\n",
    "  except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))\n",
    "  try:\n",
    "      os.mkdir(m_celeba_dir)\n",
    "  except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))\n",
    "  try:\n",
    "      os.mkdir(img_celeba_dir)\n",
    "  except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))\n",
    "\n",
    "  try:\n",
    "      shutil.copyfile(list_eval_partition_path, m_celeba_dir + r'list_eval_partition.txt')\n",
    "      shutil.copyfile(identity_celeba_path, m_celeba_dir + r'identity_CelebA.txt')\n",
    "      shutil.copyfile(list_attr_celeba_path, m_celeba_dir + r'list_attr_celeba.txt')\n",
    "      shutil.copyfile(list_bbox_celeba_path, m_celeba_dir + r'list_bbox_celeba.txt')\n",
    "      shutil.copyfile(list_landmarks_align_celeba_path, m_celeba_dir + r'list_landmarks_align_celeba.txt')\n",
    "      print(\"label files copied successfully\")\n",
    "  except OSError as e:\n",
    "      print(\"Error: %s\" % (e.strerror))"
   ],
   "metadata": {
    "id": "kS71RQSBoVsj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def delete_modified_dataset_folder():\n",
    "    try:\n",
    "        shutil.rmtree(on_fly_modified_dataset_saving_path)\n",
    "        print(\"old modified_dataset_folder removed successfully\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s\" % (e.strerror))"
   ],
   "metadata": {
    "id": "P1PogB68GbW3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function save image:\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def store_single_disk(image, file_name):\n",
    "    save_image(image, img_celeba_dir + file_name)"
   ],
   "metadata": {
    "id": "oNDQJFwuo5fM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def fix_getting_filename_problem(my_loader,i,j,batch_size):\n",
    "    if use_whole_dataset:\n",
    "        return my_loader.dataset.filename[i*batch_size + j]\n",
    "    else:\n",
    "        return my_loader.dataset.dataset.filename[i*batch_size + j]"
   ],
   "metadata": {
    "id": "u1iD7V6CUFa1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNyjQ2POq0Dj"
   },
   "outputs": [],
   "source": [
    "def convert_dataset(my_loader):\n",
    "    prog_bar = tqdm(enumerate(my_loader), total=len(my_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in prog_bar:\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.to(device)\n",
    "            gender_target = extract_targets(labels)\n",
    "            output, y1 = netAE.forward(inputs, gender_target)\n",
    "            for j in range(batch_size):\n",
    "                store_single_disk(output[j,:,:,:], fix_getting_filename_problem(my_loader,i,j,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Rqa4uBCycTu"
   },
   "outputs": [],
   "source": [
    "# using model to obfuscate dataset\n",
    "def convert_testset():\n",
    "  netAE.change_mode('use')\n",
    "  netAE.eval()\n",
    "  netAE.tune_noise(miu, coef_for_var, g_eff_val)\n",
    "  print(\"\\nConverting test images...\")\n",
    "  convert_dataset(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXSZIM8XpH-W"
   },
   "outputs": [],
   "source": [
    "# load modified dataset:\n",
    "def created_modified_dataloader():\n",
    "  m_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "  m_test_set = datasets.CelebA(root=on_fly_modified_dataset_saving_path, download=False, split='test', target_type=[\"attr\", \"identity\"], transform=m_transforms)\n",
    "  m_test_loader = torch.utils.data.DataLoader(m_test_set, batch_size=batch_size, num_workers=workers, drop_last=True)\n",
    "  return m_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Calc Accuracy\n",
    "def calcAccuracy(model, test_loader, using_index):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    y1_accuracy = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in prog_bar:\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.to(device)\n",
    "            young_target = labels[0][:, using_index]\n",
    "            young_target = young_target.to(device)\n",
    "            output = model(inputs)\n",
    "            ps_y1 = torch.exp(output)\n",
    "            top_p_y1, top_class_y1 = ps_y1.topk(1, dim=1)\n",
    "            equals_y1 = top_class_y1 == young_target.view(*top_class_y1.shape)\n",
    "            acc_y1 = equals_y1.sum().item()\n",
    "            y1_accuracy += (acc_y1 / len(equals_y1))            \n",
    "    y1_accuracy = y1_accuracy / len(test_loader)\n",
    "    return y1_accuracy"
   ],
   "metadata": {
    "id": "WOL6L2CtwpB4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test on obfuscated data\n",
    "def test_on_obfuscated_data(model, loader, index):\n",
    "  model.to(device)\n",
    "  acc = calcAccuracy(model, loader, index)\n",
    "  return acc"
   ],
   "metadata": {
    "id": "yBSgpEi0x_d4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Search for best p2r model number - loop (all values)\n",
    "\n",
    "load_saved_data = args.load_result_model_number=='True'\n",
    "last_saved_number = args.last_saved_model_number\n",
    "\n",
    "save_every_i = 5\n",
    "whole_epochs = args.whole_model_numbers\n",
    "if load_saved_data:\n",
    "  whole_epochs = args.whole_model_numbers - last_saved_number\n",
    "p2r_epoch_range = range(whole_epochs)\n",
    "\n",
    "use_g = False\n",
    "g_eff_val = -1\n",
    "miu = 0\n",
    "coef_for_var = 0\n",
    "\n",
    "adv_acc=[]\n",
    "utl_acc=[]\n",
    "\n",
    "if load_saved_data:\n",
    "  res = load_data(saving_path, args.model_number_result_file_name + str(last_saved_number))\n",
    "  adv_acc = res['adv_acc']\n",
    "  utl_acc = res['utl_acc']\n",
    "  \n",
    "if not load_saved_data:\n",
    "  for i in p2r_epoch_range:\n",
    "    if load_saved_data:\n",
    "      i = i + (last_saved_number+1) \n",
    "    print(f\"Epoch {i}/{whole_epochs}: \")\n",
    "    p2r_model_number = i\n",
    "    ae_load, adv_load, utl_load = load_models()\n",
    "    delete_modified_dataset_folder()\n",
    "    create_modified_dataset_folder()\n",
    "    convert_testset()\n",
    "    m_test_loader = created_modified_dataloader()\n",
    "    adv_accuracy = test_on_obfuscated_data(adversaryModel, m_test_loader, adv_using_index)\n",
    "    adv_acc.append(adv_accuracy)\n",
    "    utilizerModel.to(device)\n",
    "    utl_accuracy = test_on_obfuscated_data(utilizerModel, m_test_loader, using_index)\n",
    "    utl_acc.append(utl_accuracy)\n",
    "    print(f\"\\n Adversary Accuracy on Testset: {adv_accuracy:.6f}\")\n",
    "    print(f\"\\n Utilizer Accuracy on Testset: {utl_accuracy:.6f}\")\n",
    "    if i % save_every_i == 0:\n",
    "      res = {'adv_acc': adv_acc,'utl_acc': utl_acc };\n",
    "      save_data(saving_path, args.model_number_result_file_name + str(i), res)"
   ],
   "metadata": {
    "id": "IJUEsZetynTL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "matplotlib.style.use('default')\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('font', **{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "print('Select obfuscator model number plot...')\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "fig.tight_layout()\n",
    "plt.plot(adv_acc, color='red', label='A Well-Trained Network for Gender Inference')\n",
    "plt.plot(utl_acc, color='green', label='A Well-Trained Network for Smiling Inference')\n",
    "plt.xlabel(\"Obfuscator Model Number (Training Epoch Number)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlim([0,200])\n",
    "plt.legend()\n",
    "plt.grid(color = 'gray', linestyle = 'dotted', linewidth = 0.5)\n",
    "plt.savefig(saving_path + args.model_number_result_file_name + \"_plot.svg\", bbox_inches = 'tight')\n",
    "plt.savefig(saving_path + args.model_number_result_file_name + \"_plot.png\", bbox_inches = 'tight')\n",
    "plt.savefig(saving_path + args.model_number_result_file_name + \"_plot.eps\", bbox_inches = 'tight', format='eps')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "9SxCrQNsNNre"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Search for best g power (g_eff_value)\n",
    "load_saved_data = args.load_result_lambda=='True'\n",
    "g_eff_vals = [-1, -500, -1000, -2000, -3000, -4000, -5000, -10000]\n",
    "\n",
    "use_g = True\n",
    "miu = 0\n",
    "coef_for_var = 0\n",
    "p2r_model_number = args.obf_model_number  # 183 or 13\n",
    "adv_acc_p2rnum1 = []\n",
    "utl_acc_p2rnum1 = []\n",
    "\n",
    "if load_saved_data:\n",
    "  res_p2rnum1 = load_data(saving_path, args.lambda_result_file_name) # 183 -> 13\n",
    "  adv_acc_p2rnum1 = res_p2rnum1['adv_acc']\n",
    "  utl_acc_p2rnum1 = res_p2rnum1['utl_acc']\n",
    "else:  \n",
    "  for g_eff_val in g_eff_vals:\n",
    "    ae_load, adv_load, utl_load = load_models()\n",
    "    delete_modified_dataset_folder()\n",
    "    create_modified_dataset_folder()\n",
    "    convert_testset()\n",
    "    m_test_loader = created_modified_dataloader()\n",
    "    adv_accuracy = test_on_obfuscated_data(adversaryModel, m_test_loader, adv_using_index)\n",
    "    utilizerModel.to(device)\n",
    "    utl_accuracy = test_on_obfuscated_data(utilizerModel, m_test_loader, using_index)\n",
    "    adv_acc_p2rnum1.append(adv_accuracy)\n",
    "    utl_acc_p2rnum1.append(utl_accuracy)\n",
    "    print(f\"\\n Adversary Accuracy on Testset: {adv_accuracy:.6f}\")\n",
    "    print(f\"\\n Utilizer Accuracy on Testset: {utl_accuracy:.6f}\")\n",
    "  res_p2rnum1 = {'adv_acc': adv_acc_p2rnum1,'utl_acc': utl_acc_p2rnum1 };\n",
    "  save_data(saving_path, args.lambda_result_file_name, res_p2rnum1) # 183 -> 13"
   ],
   "metadata": {
    "id": "G3IA2mad2TkY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('default')\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('font', **{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)\n",
    "print('Select lambda plot...')\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "fig.tight_layout()\n",
    "plt.title(\"lambda selection for model number\" + str(args.obf_model_number)) # 183 -> 13\n",
    "plt.plot(g_eff_vals, adv_acc_p2rnum1, color='red', label='A Well-Trained Network for Gender Inference', marker='^')\n",
    "plt.plot(g_eff_vals, utl_acc_p2rnum1, color='green', label='A Well-Trained Network for Smiling Inference', marker='^')\n",
    "\n",
    "line_lambda = -3000\n",
    "line_index = 4\n",
    "if args.obf_model_number == 13:\n",
    "  line_lambda = -2000\n",
    "  line_index = 3\n",
    "\n",
    "plt.axvline(x=line_lambda, linewidth=1, color='blue', label='$\\lambda=' + str(line_lambda) + '$') # -3000 or -2000\n",
    "plt.plot([line_lambda], adv_acc_p2rnum1[line_index], color='red', marker='^') # -3000 or -2000 / 3 or 4\n",
    "plt.plot([line_lambda], utl_acc_p2rnum1[line_index], color='green', marker='^') # -3000 or -2000 / 3 or 4\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlim([-10100,+100])\n",
    "plt.legend(fontsize=8, loc=5, bbox_to_anchor=(0.706,0.5))\n",
    "plt.grid(color = 'gray', linestyle = 'dotted', linewidth = 0.5)\n",
    "plt.savefig(saving_path + args.lambda_result_file_name + \"_plot.svg\", bbox_inches = 'tight') # 183 -> 13\n",
    "plt.savefig(saving_path + args.lambda_result_file_name + \"_plot.png\", bbox_inches = 'tight') # 183 -> 13\n",
    "plt.savefig(saving_path + args.lambda_result_file_name + \"_plot.eps\", bbox_inches = 'tight', format='eps') # 183 -> 13\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "1uxK2S_bQTQJ"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}